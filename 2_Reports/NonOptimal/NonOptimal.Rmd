---
title: "Non Optimal Subsampling"
author: "Han Boissier"
date: "28 april 2017"
output:
  html_document:
    theme: journal
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	comment = NA,
	cache = TRUE,
	warning = FALSE
)
```

```{r "global-options", echo = FALSE, cache.rebuild = TRUE}
# libraries
library(ggplot2)
library(dplyr)
library(ggthemes)
library(RColorBrewer)
library(boot)
library(parallel)

# Custom QQ plot function
gg_qq <- function(x, distribution = "norm", ..., line.estimate = NULL, conf = 0.95,
                  labels = names(x), title = NULL){
  q.function <- eval(parse(text = paste0("q", distribution)))
  d.function <- eval(parse(text = paste0("d", distribution)))
  x <- na.omit(x)
  ord <- order(x)
  n <- length(x)
  P <- ppoints(length(x))
  df <- data.frame(ord.x = x[ord], z = q.function(P, ...))

  if(is.null(line.estimate)){
    Q.x <- quantile(df$ord.x, c(0.25, 0.75))
    Q.z <- q.function(c(0.25, 0.75), ...)
    b <- diff(Q.x)/diff(Q.z)
    coef <- c(Q.x[1] - b * Q.z[1], b)
  } else {
    coef <- coef(line.estimate(ord.x ~ z))
  }

  zz <- qnorm(1 - (1 - conf)/2)
  SE <- (coef[2]/d.function(df$z)) * sqrt(P * (1 - P)/n)
  fit.value <- coef[1] + coef[2] * df$z
  df$upper <- fit.value + zz * SE
  df$lower <- fit.value - zz * SE

  if(!is.null(labels)){ 
    df$label <- ifelse(df$ord.x > df$upper | df$ord.x < df$lower, labels[ord],"")
    }

  p <- ggplot(df, aes(x=z, y=ord.x)) +
    geom_point() + 
    geom_abline(intercept = coef[1], slope = coef[2]) +
    geom_ribbon(aes(ymin = lower, ymax = upper), alpha=0.2) + 
    scale_x_continuous(name = 'Normal Quantiles') + 
    scale_y_continuous(name = 'Sample Quantiles') +
      ggtitle("QQ-plot", subtitle = title)
  if(!is.null(labels)) p <- p + geom_text( aes(label = label))
  #print(p)
  return(p)
}

# Custom function to calculate CI coverage
checkCoverage <- function(estimate, variance, TrueValue, level = 0.95){
  CIlow <- estimate - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(variance))
  CIup <- estimate + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(variance))
  return(ifelse(TrueValue >= CIlow && TrueValue <= CIup, 1, 0))
}

# Bootstrap function
BootCI <- function(OrigData, type = 'bca'){
  # Function to calculate mean statistic
  meanBoot <- function(data, indices){
    d <- data[indices]
    return(mean(d))
  }
  # Bootstrap and CI
  boot_result <- boot(data = OrigData, statistic = meanBoot, R = 1000)
  boot_ci <- boot.ci(boot_result, type = type)
  return(data.frame('lwrBoot' = boot_ci$bca[4], 'uprboot' = boot_ci$bca[5]))
}

# Construct a function that can be used over workers in parallel
BootParCI <- function(sID, datLoc, SCEN){
  # Bootstrap function
  BootCI <- function(OrigData, type = 'bca'){
    # Function to calculate mean statistic
    meanBoot <- function(data, indices){
      d <- data[indices]
      return(mean(d))
    }
    # Bootstrap and CI
    boot_result <- boot(data = OrigData, statistic = meanBoot, R = 1000)
    boot_ci <- boot.ci(boot_result, type = type)
    return(data.frame('lwrBoot' = boot_ci$bca[4], 'uprboot' = boot_ci$bca[5]))
  }
  
  # Names of the columns in data files, according to model
  S1_colnames <- c("beta", "sVariance", "K", "nRSloops", "TrueBeta")
  S3_colnames <- c("beta.X_smartph_hrs", "beta.X_sex", 
                 "beta.X_econArea",  "beta.X_ethnic", 
                 "sVariance.X_smartph_hrs", "sVariance.X_sex",
                 "sVariance.X_econArea", 
                 "sVariance.X_ethnic",  "K", "nRSloops", "TrueBeta")
  if(SCEN == 1){
    assign(x = 'generic_colnames', value = S1_colnames)
  }
  if(SCEN == 3){
    assign(x = 'generic_colnames', value = S3_colnames)
  }
 
  # Read values from one simulation
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', sID, '_SCEN_', SCEN, '.txt'),
                 col.names = generic_colnames, header = TRUE) %>% 
                   tbl_df()
  # Rename the beta.X_smartph_hrs column to beta
  if(SCEN == 3){
    ValSim <- rename(ValSim, beta = beta.X_smartph_hrs)
  }
  
  # Run bootstrap, construct CI, append the true value, then check whether this value is in CI
  PercCI <- ValSim %>% group_by(K, nRSloops) %>% 
    do(bind_cols(BootCI(OrigData = .$beta))) %>%
    mutate(TrueBeta = as.numeric(slice(select(ValSim, TrueBeta), 1))) %>%
    do(slice(., 1)) %>% rowwise() %>% mutate(coverage = ifelse(TrueBeta >= lwrBoot && TrueBeta <= uprboot, 1, 0)) %>% 
      mutate(sim = sID) 
  return(PercCI)
}

# Get info on machine we are compiling
OS <- Sys.info()['machine']
if(OS == 'x86_64'){
  MACHINE <- "MAC"
  wd <- '/Users/hanbossier/Dropbox/Mastat/Thesis/BigDataPIM'
}else{
  MACHINE <- "DOS"
  wd <- 'D:/Users/Han/Dropbox/Mastat/Thesis/BigDataPIM'
}

# location of data
if(MACHINE == "MAC"){
datLoc <- "/Volumes/1_5_TB_Han_HDD/Mastat/Thesis/BigDataPIM/NonOptimal/"  
}
if(MACHINE == "DOS"){
datLoc <- ""  
}
```


## Introduction
In this report, we consider a non-optimal subsampling approach when fitting a PIM on a large dataset. We first simulate a dataset (sample size, $N = 250.000$) under the normal linear model. We consider several models in which we increase the complexity. Then we apply a non-optimal subsampling algorithm. We record the estimated $\beta$ parameters and the time needed (on the HPC infastructure) to estimate these. In the first section, we briefly explain the algorithm. Next we give the simlation details. Finally, we look at the results.  

## Non Optimal Subsampling
The main problem when fitting a Probabilistic Index Model (PIM) on a large dataset is the time needed to estimate the parameters. In order to limit the computational time, we first explore a non-optimal subsampling approach. Consider a set of subsampling probabilites $\pi_i, i = 1,...,n$ assigned to all data points. We define $\boldsymbol{\pi} = \{\pi_i\}_{i=1}^n$. Next we apply the following algorithm:

* **1) sample:** draw a random sample $K < N$ from the original dataset with probability $\boldsymbol{\pi}$
* **2) estimate:** estimate the $\beta$ parameters using a PIM on the subset of the data. 
* **3) iterate:** repeat step 1 for **B** times.

In this report, we consider the nonuniform subsampling probability $\boldsymbol{\pi}^{\text{UNI}} = \{\pi_i = n^{-1}\}_{i=1}^n$ in step 1. 


## Simulations

### Model 1

#### Data generation

```{r echo = FALSE}
u <- 1
alpha <- 5
sigma <- 1
trueBeta <- alpha/(sqrt(2) * sigma)
```

We first generate data under the linear model:

$$
Y_i = \alpha X_i + \varepsilon_i 
$$
with $i = 1,...,n$ and $\varepsilon_i$ are i.i.d. $N({0,\sigma^2)}$ with $\sigma^2 = 1$. We consider a sample size $n = 250.000$. The predictor (*X*) is uniformly spaced between $[0.1,1]$. The value $\alpha$ is set to 5. Using the probit link function, the corresponding PIM can be expressed as:

$$
\Phi^{-1}[P(Y\preceq Y'|X,X')] = \beta(X' - X)
$$
where $\beta = \alpha/\sqrt{2\sigma^2}$. Hence the true value for $\beta$ equals ```r 5/(sqrt(2 * 1))```.


### Model 2
For the second model, we choose $\alpha = 10$, a predictor (*X*) uniformly spaced between $[0.1,10]$ and $\sigma^2 = 25$. Hence, the true value for $\beta =$ ```r 10/(sqrt(2 * 25))```.

### Model 3
The third model is a multiple regression model in which we take parameters from the analysis done in Przybylski and Weinstein (2017). In their study, the authors used linear regression to model the effect of (among other variables) smartphone usage in the weekdays and weekends on self-reported mental well being. To control for confounding effects, they included variables for sex, whether the participant was located in economic deprived areas and the ethnic background (minority group yes/no) in the model. The full model is given as:

$$
Y_i = \mu + \alpha_1 X_i + \gamma_1 Z_{1i} + \gamma_2 Z_{2i} + \gamma_3 Z_{31i} + \varepsilon_i 
$$

Based on a linear regression, we find the regression parameters, the proportions of the covariates ($Z_j$), the range of the predictor ($X$, smartphone usage during weekdays measured on a Likert scale from 0-7) and the spread of the observations ($\sigma = 9.51$). These parameters are then used to generate normally i.i.d. data. 


```{r "dataWB", collapse=TRUE, cache = TRUE}
dataWB <- read.csv(file = paste(wd, '/OSF_Przybylski2017/data.csv', sep = ''))

# Smartphone screen usage during weekdays, corrected for sex, economic area and minority status
dataWB %>% select(mwbi, sp_wd, male, minority, deprived) %>% 
  filter(complete.cases(.)) %>%
  mutate(sp_wd_sq = sp_wd**2) %>% 
  lm(mwbi ~ sp_wd + male + minority + deprived, data = .) %>%
  summary(.)

# Spread of data
sd(dataWB$mwbi, na.rm = TRUE)

# proportion of gender, minority and deprived
mean(dataWB$male)
mean(dataWB$deprived)
mean(dataWB$minority)
```

Then for each simulation, we generate a dataset through:
```{r "example-model-3"}
# Sample size
n <- 250000
# Regression parameters
alpha_0 <- 46.72
alpha_1 <- -0.43
# Control variables: sex (0 = female, 1 = male)
sex <- c(0,1)
  alpha_sex <- 4.55
# Economic area (deprived = 1, otherwise 0)
econArea <- c(0,1)
  alphaEcon <- -0.45
# Ethnic background (white = 0, otherwise 1)
ethnic <- c(0,1)
  alpha_ethnic <- 0.30
# Predictor is a discrete scale from 0 --> 7
u <- 7
# Sigma based on dataset
sigma <- 9.51

# Seed: note, in simulations, this seed depends on ID of simulation!
set.seed(123)

# Predictors: proportions come from observed proportions in study of mental well being
X_smartph_hrs <- sample(x = c(0:u), size = n, replace = TRUE)
X_sex <- sample(x = sex, size = n, replace = TRUE, prob = c(1-0.4758, 0.4758))
X_econArea <- sample(x = econArea, size = n, replace = TRUE, prob = c(1-0.4348, 0.4348))
X_ethnic <- sample(x = ethnic, size = n, replace = TRUE, prob = c(1-0.2408, 0.2408))

 # Observed data
Y <- alpha_0 + alpha_1*X_smartph_hrs + alpha_sex*X_sex +
       alphaEcon*X_econArea + alpha_ethnic*X_ethnic +
        rnorm(n = n, mean = 0, sd = sigma)
```
The true value for $\beta =$ ```r -0.43/(sqrt(2) * 9.51)```.

For comparisson, we plot the distribution of mental well being in the dataset (left) and $n = 250.000$ simulated datapoints (right). 

```{r "histograms", fig.width=10, fig.align="center"}
par(mfrow = c(1,2))
hist(dataWB$mwbi, main = "Mental well being in Przybylski and Weinstein (2017)", xlab = "")
hist(Y, main = "Simulated mental well being", xlab = "")
```

```{r echo = FALSE}
par(mfrow = c(1,1))
```

> Note that we did not introduce the observed skeweness in the real dataset. 

### Simulation parameters
We vary the amount of iterations **B** as well as the number of selected samples **K** in each iteration to find the optimal balance in bias and computation time. Both **B** and **K** range from 10 to 1000. We consider 10 levels between these points for each parameter. Hence we have $10 \times 10 = 100$ scenarios. 

```{r "simulation parameters", echo = FALSE}
# number of simulations 
nsim <- 1000

# Confidence level (for CI section)
level <- 0.95

# Vector of number of resampling loops
nRSloops_vec <- floor(seq(10,1000,length.out = 10))

# Vector of number of selected datapoints K per iteraton 
K_vec <- floor(seq(10,1000,length.out = 10))

# Number of pairs/combinations between number of resampling loops and sampled data 
pairs <- expand.grid(B = nRSloops_vec, K = K_vec)
cbind(head(pairs),' ...' = c(' ...',' ...',' ...',' ...',' ...',' ...'),tail(pairs))
```

We simulate each combination of **B** and **K** for 1000 times.

## Results

### Model 1

Parameters:
```{r "model-1-parameters", cache.rebuild = TRUE}
u <- 1
alpha <- 5
sigma <- 1
trueBeta <- alpha/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 1
```

#### Load in data

Start with estimated $\beta$:
```{r echo = FALSE}
# Data frame with all values over all simulations
valuesAllSim <- timeAllSim <- data.frame() %>% tbl_df()

# Progress
progress <- floor(seq(1,nsim,length.out = 11)[-1])
```


```{r "load-beta-model-1", results = 'hide', cache = TRUE}
# load in estimated beta values
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
               col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% tbl_df() %>%
      group_by(K, nRSloops) %>% summarise(avBeta = mean(beta)) %>% 
      mutate(sim = i), silent = TRUE)
  if(class(ValSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  valuesAllSim <- bind_rows(valuesAllSim, ValSim)
}
```

Same for computational time:
```{r "load-time-model-1", results = 'hide', cache = TRUE}
# load in computational time
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: computational time 
  TimeSim <-  try(read.table(file = paste0(datLoc, 'uni_time_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
              col.names = c("minutes"), header = TRUE) %>% tbl_df() %>%
              bind_cols(., pairs) %>% 
              mutate(sim = i), silent = TRUE)
    if(class(TimeSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  timeAllSim <- bind_rows(timeAllSim, TimeSim)
}
```

#### Bias vs computational time

Quick summary:
```{r collapse = TRUE}
tbl_df(valuesAllSim)
group_by(valuesAllSim, K, nRSloops) %>% summarise(avBeta = mean(avBeta)) %>% summary()
```

Plotting the estimated $\beta$ parameters for each simulation through facets on the number of iterations (**B**).

```{r "non-evaluated-plot-model-1", eval = FALSE, echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = factor(K), y = avBeta, group = factor(nRSloops))) + 
  geom_jitter(aes(colour = factor(nRSloops)), height = 0) +
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_y_continuous(name = expression(hat(beta))) +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Estimated beta") +
  geom_hline(yintercept = trueBeta, size = 1) + 
  theme_minimal()
```


```{r "beta-model-1", echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = K, y = avBeta)) + 
  geom_point(size = 0.7, colour = "#67a9cf") + 
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = expression(hat(beta))) +
  geom_line(data = mutate(.data = valuesAllSim, TrueValue = trueBeta, Label = "True Value of Beta"),
             aes(y = TrueValue, linetype = Label), size = 0.7, show.legend = TRUE) +
  scale_linetype_discrete(name = "") +
  ggtitle("Estimated beta parameter given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

Plotting the computational time.

```{r "time-model-1", echo=FALSE, fig.align="center"}
TimeToPlot <- timeAllSim %>% group_by(B,K) %>% summarise(AvgMin = mean(minutes))
  TimeToPlot$B <- factor(TimeToPlot$B)
  TimeToPlot$K <- factor(TimeToPlot$K)

ggplot(data = TimeToPlot, aes(x = K, y = AvgMin, group = B)) + 
  geom_line(aes(colour = B),size = 1) +
  scale_y_continuous(name = "Minutes",
                     limits = c(0,max(TimeToPlot$AvgMin) + 2)) + 
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Average computational time to estimate PIM on HPC") +
  theme_minimal()

```

> Note: this is run on the HPC which will runs faster than regular PCs.

#### Combine bias and time
Can we combine time and bias in one graph? Maybe create a summary measure which involves a penalty term for the required time to estimate the parameters. 

#### Distribution of estimator
The figures below show the QQ-plots of $\widehat\beta$ associated with model 1 when having 500 samples in one iteration and applying 200 iterations.

```{r "not-evaluated-QQ-plot-model-1", eval = FALSE, echo = FALSE}
valuesAllSim %>% filter(K == 340 & nRSloops == 230) %>% ggplot(aes(sample =
avBeta)) + stat_qq() + ggtitle('Normal QQ-plot when K = 340 and B = 230')

valuesAllSim %>% filter(K == 230 & nRSloops == 120) %>% ggplot(aes(sample =
avBeta)) + stat_qq()
```


```{r "QQplot-model-1", fig.show='hold', fig.width=10}
for(j in 1:length(nRSloops_vec)){
  K_temp <- K_vec[j]
  B_temp <- nRSloops_vec[j]
  assign(paste0("Plot", j),
    valuesAllSim %>% filter(K == K_temp & nRSloops == B_temp) %>% 
    select(avBeta) %>% unlist(.) %>% 
    as.numeric() %>% gg_qq(x = ., title = paste0('K = ', K_temp, ' and B = ', B_temp))
  )
}

cowplot::plot_grid(Plot1, Plot2, Plot3, Plot4, Plot5, Plot6, labels = c("A", "B", "C", "D", "E", "F"), ncol = 3)
cowplot::plot_grid(Plot7, Plot8, Plot9, Plot10, labels = c("G", "H", "I", "J"), ncol = 3)
```


#### Emperical CI coverage

##### Within iteration sandwich estimator

In this section, we calculate for every iteration (**B**) the 95% confidence interval. Within each iteration, we can check its nominal coverage level and then average over all simulations. Note, that this is just equal to running multiple simulations in which you check the asymptotic properties of the sandwich estimator and not related to our research question! 

```{r calculate-CI-model-1}
# Empty data frame
valuesAvgCov <- data.frame() %>% tbl_df()

# load in estimated beta values and calculate coverage
for(i in 1:nsim){
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
                 col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                   tbl_df(), silent = TRUE)
  if(class(ValSim)[1] == "try-error"){
    print(i);next
  }
  # Calculate coverage per simulation: estimate +/- (1.96 * sqrt(sandwichVariance)) for 95% CI
  WithCoverage <- mutate(ValSim, CIlow = beta - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>%
              mutate(CIup = beta + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>% rowwise() %>%
              mutate(coverage = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0))
      
  # Average coverage per simulation
  AvgCov <- WithCoverage %>% ungroup() %>% group_by(K, nRSloops) %>%
      summarise(AvgCov = mean(coverage, na.rm = TRUE)) %>% 
      mutate(sim = i)
  
  # Add to data frame with all simulations
  valuesAvgCov <- bind_rows(valuesAvgCov, AvgCov)
}
```

Now we proceed to plot the average CI level per **B** and **K**.

```{r "plot-CI-model-1", echo = FALSE, fig.align = 'center'}
ggplot(valuesAvgCov, aes(x = K, y = AvgCov)) + 
  geom_point(size = 0.7, colour = "#67a9cf") +
  geom_hline(aes(yintercept = level), size = 0.7) +
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = "95% CI coverage") +
  scale_linetype_discrete(name = "") +
  ggtitle("Average CI coverage given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

For more detail, we refer to the table below with the average emperical coverage per **B** (columns) and **K** (rows).
```{r "table-EC-sandwich-model-1", echo = FALSE}
resTable_tmp <- valuesAvgCov %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(AvgCov), 3))
resSDtable <- valuesAvgCov %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(AvgCov), 3), SD = round(sd(AvgCov), 2))
#knitr::kable(valuesAvgCov %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(AvgCov), 2), SD = round(sd(AvgCov), 2)))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```


##### Bias-corrected bootstrap based CI

Now we turn to the more interesting question. How can we create confidence intervals around the average $\beta$ PIM estimate obtained in one iteration in the non optimal resampling procedure? We suggest to use a bootstrap procedure. Let us try with the bias-corrected and accelerated bootstrap (BCa) which adjusts both for bias and skeweness. Note, this might introduce extra computation time. To get the results here, we run all simulations in parallel.
Note that we defined the function to calculate the coverages (*BootParCI*) earlier.

```{r "percentile-CI-model-1", eval = FALSE, echo = FALSE}
# Empty data frame
valuePercCI <- data.frame() %>% tbl_df()

# load in estimated beta values and calculate coverage
for(i in 1:nsim){
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <- try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
                 col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                   tbl_df(), silent = TRUE)
  if(class(ValSim)[1] == "try-error"){
    print(i);next
  }
  
  # Create the percentile based CI lower and upper bound, 
    # then take one value for each combination of B and K
    # then check whether true value is between bounds
  PercCI <- ValSim %>% group_by(K, nRSloops) %>% 
    do(mutate(.,PercLower = quantile(x = .$beta, probs = c((1-level)/2)),
                PercUpper = quantile(x = .$beta, probs = c(1-(1-level)/2)))) %>%
    do(slice(., 1)) %>% rowwise() %>% mutate(coverage = ifelse(TrueBeta >= PercLower && TrueBeta <= PercUpper, 1, 0)) %>% 
      mutate(sim = i) 
  
  # Add to data frame with all simulations
  valuePercCI <- bind_rows(valuePercCI, PercCI)
}
```


```{r "percentile-CI-model-1-sequential", eval = FALSE, echo = FALSE}
# Empty data frame
valuePercCI <- data.frame() %>% tbl_df()

# load in estimated beta values and calculate coverage
for(i in 1:nsim){
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <- try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
                 col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                   tbl_df(), silent = TRUE)
  if(class(ValSim)[1] == "try-error"){
    print(i);next
  }
  
  # Run bootstrap, construct CI, append the true value, then check whether this value is in CI
  PercCI <- ValSim %>% group_by(K, nRSloops) %>% 
    do(bind_cols(BootCI(OrigData = .$beta))) %>%
    mutate(TrueBeta = as.numeric(slice(select(ValSim, TrueBeta), 1))) %>%
    do(slice(., 1)) %>% rowwise() %>% mutate(coverage = ifelse(TrueBeta >= lwrBoot && TrueBeta <= uprboot, 1, 0)) %>% 
      mutate(sim = i) 
  
  # Add to data frame with all simulations
  valuePercCI <- bind_rows(valuePercCI, PercCI)
}
```


```{r "percentile-CI-model-1-parallel", cache = TRUE, results = 'hide'}
# Detect and start the workers
P <- detectCores(logical = FALSE) # physical cores
cl <- makeCluster(P)

# Initialize them with the libraries
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(boot))

# Run in parallel and append results
boot_par_results <- clusterApply(cl, 1:nsim, fun = BootParCI, datLoc = datLoc, SCEN = SCEN)
boot_CI_coverage <- do.call(rbind, boot_par_results)

# Stop the workers
stopCluster(cl)
```

```{r "table-EC-bootstrap-model-1", echo = FALSE}
resTable_tmp <- boot_CI_coverage %>% ungroup() %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(coverage), 3))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```

> Quite bad coverages!


```{r "test-code", eval = FALSE, echo = FALSE}
# Bootstrap function
BootCI <- function(OrigData, type = 'bca'){
  # Function to calculate mean statistic
  meanBoot <- function(data, indices){
    d <- data[indices]
    return(mean(d))
  }
  # Bootstrap and CI
  boot_result <- boot(data = OrigData, statistic = meanBoot, R = 1000)
  boot_ci <- boot.ci(boot_result, type = type)
  return(data.frame('lwrBoot' = boot_ci$bca[4], 'uprboot' = boot_ci$bca[5]))
}
str(boot_result)
mean(boot_result$t)
boot_ci$bca[5] - boot_result$t0
boot_result$t0 - boot_ci$bca[4]

boot_ci$bca[5] - mean(boot_result$t)
mean(boot_result$t) - boot_ci$bca[4]

  #View(valuePercCI)
  table(valuePercCI$coverage)

ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', sample(x = 1:1000, size = 1), '_SCEN_', SCEN, '.txt'),
               col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                 tbl_df()
test <- ValSim %>% group_by(K, nRSloops) %>% filter(K == 230 & nRSloops == 230)
BootResults <- boot(data = test$beta, statistic = meanBoot, R = 1000)
boot.ci(BootResults, type = 'bca')
TrueBeta <- 3.53534

boot.ci(BootResults, type = 'bca')$bca
BootResults
BootResults$t %>% sd()
mean(BootResults$t) - BootResults$t0


str(boot.ci(BootResults, type = 'bca'))
unlist(boot.ci(BootResults, type = 'bca'))[c('bca4', 'bca5')]
boot.ci(BootResults, type = 'bca')$bca[4]


bootVector <- sandwichVector <- sandwichIndVector <- lowBoundVarVector <- SEbootVector <- SEVector <- upBoundVarVector <- c()
TrueBeta <- 3.53534
for(i in 1:250){
  if(i == 250) print('25%')
  if(i == 500) print('50%')
  if(i == 750) print('75%')
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
           tbl_df()
  test <- ValSim %>% group_by(K, nRSloops) %>% filter(K == 10 & nRSloops == 10)
  
  # Bootstrap
  BootResults <- boot(data = test$beta, statistic = meanBoot, R = 1000)
  boot_ci <- boot.ci(BootResults, type = 'norm')
  bootVector <- c(bootVector, ifelse(TrueBeta >= boot_ci$norm[2] & TrueBeta <= boot_ci$norm[3], 1, 0))
  
  # SE of bootstrap
  seBootLow <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(BootResults$t))
  seBootUp <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(BootResults$t))
  SEbootVector <- c(SEbootVector, ifelse(TrueBeta >= seBootLow & TrueBeta <= seBootUp, 1, 0))
  
  # SE through sd of resamples
  seLow <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(test$beta))
  seUp <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(test$beta))
  SEVector <- c(SEVector, ifelse(TrueBeta >= seLow & TrueBeta <= seUp, 1, 0))
  
  # Mean of sandwich estimated variance
  sLow <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(mean(test$sVariance)))
  sUp <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(mean(test$sVariance)))
  sandwichVector <- c(sandwichVector, ifelse(TrueBeta >= sLow & TrueBeta <= sUp, 1, 0))
  
  # Mean of CI bounds
  WithCI <- mutate(test, CIlow = beta - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>%
              mutate(CIup = beta + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance)))
  sandwichIndVector <- c(sandwichIndVector, ifelse(TrueBeta >= mean(WithCI$CIlow) & TrueBeta <= mean(WithCI$CIup), 1, 0))
  
  # Lower bound of bootstrap on sandwich variance
  varBoot <- boot(data = test$sVariance, statistic = meanBoot, R = 1000)
  lowerbound <- boot.ci(varBoot, type = 'norm')$norm[2]
  lowBoundVar <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(lowerbound))
  upBoundVar <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(lowerbound))
  lowBoundVarVector <- c(lowBoundVarVector, ifelse(TrueBeta >= lowBoundVar & TrueBeta <= upBoundVar, 1, 0))
  
  # Upper bound of bootstrap on sandwich variance
  upperbound <- boot.ci(varBoot, type = 'norm')$norm[3]
  lowUPBoundVar <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(upperbound))
  upUPBoundVar <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(upperbound))
  upBoundVarVector <- c(upBoundVarVector, ifelse(TrueBeta >= lowUPBoundVar & TrueBeta <= upUPBoundVar, 1, 0))
}

mean(bootVector)
mean(SEbootVector)
mean(SEVector)
mean(sandwichVector)
mean(sandwichIndVector)
mean(lowBoundVarVector)
mean(upBoundVarVector)

avBeta <- c()
for(i in 1:250){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
           tbl_df()
  test <- ValSim %>% group_by(K, nRSloops) %>% filter(K == 340 & nRSloops == 120)
  avBeta <- c(avBeta, mean(test$beta))
}
mean(avBeta)
var(avBeta)  

  
# Check bootstrap on original dataset versus variance estimator 
# Generate predictor
X <- runif(n = 230, min = 0.1, max = 10)
 # Generate data
Y <- alpha*X + rnorm(n = 230, mean = 0, sd = sigma)
 # In data frame
OrigData <- data.frame(Y = Y, X = X)

PIMfit <- pim(formula = Y ~ X, data = OrigData,
              link = 'probit', model = 'difference')
PIMfit@vcov
bootPIM <- function(data, indices){
  d <- data[indices, ]
  return(pim(formula = Y ~ X, data = d,
              link = 'probit', model = 'difference')@coef)
}
bootPIM_results <- boot(data = OrigData, statistic = bootPIM, R = 1000)
boot.ci(bootPIM_results, type = "bca")
PIMfit@coef + (1.96 * sqrt(PIMfit@vcov))
PIMfit@coef - (1.96 * sqrt(PIMfit@vcov))

bootLM <-  function(data, indices){
  d <- data[indices, ]
  return(lm(Y ~ X, data = d)$coef[2])
}
summary(lm(Y ~ X, data = OrigData))
bootLM_results <- boot(data = OrigData, statistic = bootLM, R = 1000)
confint(lm(Y ~ X, data = OrigData), level = 0.95)
boot.ci(bootLM_results, type = "bca")


X <- runif(n = 1000000, min = 0.1, max = 10)
 # Generate data
Y <- alpha*X + rnorm(n = 1000000, mean = 0, sd = sigma)
 # In data frame
OrigData <- data.frame(Y = Y, X = X)
summary(lm(Y ~ X, data = OrigData))


mean.fun <- function(d, i) 
{    m <- mean(d$hours[i])
     n <- length(i)
     v <- (n-1)*var(d$hours[i])/n^2
     c(m, v)
}
air.boot <- boot(aircondit, mean.fun, R = 999)
boot.ci(air.boot, type = c("norm", "basic", "perc", "stud"))

mean.fun <- function(d, i) 
{    m <- mean(d$hours[i])
     n <- length(i)
     v <- (n-1)*var(d$hours[i])/n^2
     c(m)
}
air.boot <- boot(aircondit, mean.fun, R = 999)



WithCoverage <- mutate(test, CIlow = beta - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>%
              mutate(CIup = beta + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>% rowwise() %>%
              mutate(coverage = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0))
WithCoverage$coverage %>% mean()
mean(test$beta)
mean(test$sVariance)
BootResults$t %>% sd()
sqrt(0.05561626)
BootCI(test$beta)
OrigData <- test$beta

ValSim %>% group_by(K, nRSloops) %>% 
    do(mutate_(.,BootCI(OrigData = .$beta)))

t1 <- Sys.time()
ValSim %>% filter(K == 230 & nRSloops == 230) %>% group_by(K, nRSloops) %>% 
  do(bind_cols(BootCI(OrigData = .$beta)))
Sys.time() - t1

t1 <- Sys.time()
ValSim %>% group_by(K, nRSloops) %>% 
  do(bind_cols(BootCI(OrigData = .$beta)))
Sys.time() - t1

TrueBeta_tmp <- as.numeric(slice(select(ValSim, TrueBeta), 1))
ValSim %>% filter(K == c(10,120) & nRSloops == c(10,120)) %>%  group_by(K, nRSloops) %>% 
  do(bind_cols(BootCI(OrigData = .$beta))) %>% 
  mutate(TrueBeta = TrueBeta_tmp)


  
ValSim %>% filter(K == c(10,120) & nRSloops == c(10,120)) %>% 
  group_by(K, nRSloops) %>% 
  slice_rows() %>%
  by_slice(~ BootCI(.x$beta), .collate = "rows")

  by_slice(~ describe(.x$price), .collate = "rows")
do(BootCI(OrigData = .$beta))
?by_slice
```


### Model 2

Parameters:
```{r "model-2-parameters", cache.rebuild = TRUE}
u <- 10
alpha <- 10
sigma <- 5
trueBeta <- alpha/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 2
```

#### Load in data

Start with estimated $\beta$:
```{r echo = FALSE}
# parameters 
nsim <- 500

# Data frame with all values over 1000 simulations
valuesAllSim <- timeAllSim <- data.frame() %>% tbl_df()

# Progress
progress <- floor(seq(1,nsim,length.out = 11)[-1])
```


```{r "load-beta-model-2", results = 'hide', cache = TRUE}
# load in estimated beta values
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
               col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% tbl_df() %>%
      group_by(K, nRSloops) %>% summarise(avBeta = mean(beta)) %>% 
      mutate(sim = i), silent = TRUE)
  if(class(ValSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  valuesAllSim <- bind_rows(valuesAllSim, ValSim)
}
```

Same for computational time:
```{r "load-time-model-2", results = 'hide', cache = TRUE}
# load in computational time
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: computational time 
  TimeSim <-  try(read.table(file = paste0(datLoc, 'uni_time_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
              col.names = c("minutes"), header = TRUE) %>% tbl_df() %>%
              bind_cols(., pairs) %>% 
              mutate(sim = i), silent = TRUE)
    if(class(TimeSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  timeAllSim <- bind_rows(timeAllSim, TimeSim)
}
```


#### Bias vs computational time

Quick summary:
```{r collapse = TRUE}
tbl_df(valuesAllSim)
group_by(valuesAllSim, K, nRSloops) %>% summarise(avBeta = mean(avBeta)) %>% summary()
```

Plotting the estimated $\beta$ parameters for each simulation through facets on the number of iterations (**B**).

```{r "beta-model-2", echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = K, y = avBeta)) + 
  geom_point(size = 0.7, colour = "#67a9cf") + 
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = expression(hat(beta))) +
  geom_line(data = mutate(.data = valuesAllSim, TrueValue = trueBeta, Label = "True Value of Beta"),
             aes(y = TrueValue, linetype = Label), size = 0.7, show.legend = TRUE) +
  scale_linetype_discrete(name = "") +
  ggtitle("Estimated beta parameter given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

Plotting the computational time.

```{r "time-model-2", echo=FALSE, fig.align="center"}
TimeToPlot <- timeAllSim %>% group_by(B,K) %>% summarise(AvgMin = mean(minutes))
  TimeToPlot$B <- factor(TimeToPlot$B)
  TimeToPlot$K <- factor(TimeToPlot$K)

ggplot(data = TimeToPlot, aes(x = K, y = AvgMin, group = B)) + 
  geom_line(aes(colour = B),size = 1) +
  scale_y_continuous(name = "Minutes",
                     limits = c(0,max(TimeToPlot$AvgMin) + 2)) + 
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Average computational time to estimate PIM on HPC") +
  theme_minimal()
```

> These data generating parameters do not work!

### Model 3

Parameters:
```{r "model-3-parameters", cache.rebuild = TRUE}
alpha_1 <- -0.43
# Sigma based on dataset
sigma <- 9.51
trueBeta <- alpha_1/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 3
```

#### Load in data

Start with estimated $\beta$:
```{r echo = FALSE}
# parameters 
nsim <- 1000

# Data frame with all values over 1000 simulations
valuesAllSim <- timeAllSim <- data.frame() %>% tbl_df()

# Progress
progress <- floor(seq(1,nsim,length.out = 11)[-1])
```


```{r "load-beta-model-3", results = 'hide', cache = TRUE}
S3_colnames <- c("beta.X_smartph_hrs", "beta.X_sex", 
                 "beta.X_econArea",  "beta.X_ethnic", 
                 "sVariance.X_smartph_hrs", "sVariance.X_sex",
                 "sVariance.X_econArea", 
                 "sVariance.X_ethnic",  "K", "nRSloops", "TrueBeta")

# load in estimated beta values
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_3.txt'),
                        col.names = S3_colnames, header = TRUE) %>% tbl_df()  %>%
        group_by(K, nRSloops) %>% summarise_all(mean) %>%
        mutate(sim = i)
  
   if(class(ValSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  valuesAllSim <- bind_rows(valuesAllSim, ValSim)
}
```

Same for computational time:
```{r results = 'hide', cache = TRUE}
# load in computational time
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: computational time 
  TimeSim <-  read.table(file = paste0(datLoc, 'uni_time_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
              col.names = c("minutes"), header = TRUE) %>% tbl_df() %>%
              bind_cols(., pairs) %>% 
              mutate(sim = i)
  
  # Add to data frame with all simulations
  timeAllSim <- bind_rows(timeAllSim, TimeSim)
}
```

#### Bias vs computational time

Quick summary:
```{r collapse = TRUE}
group_by(valuesAllSim, K, nRSloops) %>% summarise(avBeta = mean(beta.X_smartph_hrs)) %>% summary()
```

Plotting the estimated $\beta$ parameters for each simulation through facetting on the number of iterations (**B**).


```{r "beta-model-3", echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = K, y = beta.X_smartph_hrs)) + 
  geom_point(size = 0.7, colour = "#67a9cf") + 
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = expression(hat(beta))) +
  geom_line(data = mutate(.data = valuesAllSim, TrueValue = trueBeta, Label = "True Value of Beta"),
             aes(y = TrueValue, linetype = Label), size = 0.7, show.legend = TRUE) +
  scale_linetype_discrete(name = "") +
  ggtitle("Estimated beta parameter given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

Plotting the computational time.

```{r "time-model-3", echo=FALSE, fig.align="center"}
TimeToPlot <- timeAllSim %>% group_by(B,K) %>% summarise(AvgMin = mean(minutes))
  TimeToPlot$B <- factor(TimeToPlot$B)
  TimeToPlot$K <- factor(TimeToPlot$K)

ggplot(data = TimeToPlot, aes(x = K, y = AvgMin, group = B)) + 
  geom_line(aes(colour = B),size = 1) +
  scale_y_continuous(name = "Minutes",
                     limits = c(0,max(TimeToPlot$AvgMin) + 2)) + 
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Average computational time to estimate PIM on HPC") +
  theme_minimal()
```

#### Distribution of estimator

```{r "QQplot-model-3", fig.show='hold', fig.width=10}
for(j in 1:length(nRSloops_vec)){
  K_temp <- K_vec[j]
  B_temp <- nRSloops_vec[j]
  assign(paste0("Plot", j),
    valuesAllSim %>% filter(K == K_temp & nRSloops == B_temp) %>% 
    select(beta.X_smartph_hrs) %>% unlist(.) %>% 
    as.numeric() %>% gg_qq(x = ., title = paste0('K = ', K_temp, ' and B = ', B_temp))
  )
}

cowplot::plot_grid(Plot1, Plot2, Plot3, Plot4, Plot5, Plot6, labels = c("A", "B", "C", "D", "E", "F"), ncol = 3)
cowplot::plot_grid(Plot7, Plot8, Plot9, Plot10, labels = c("G", "H", "I", "J"), ncol = 3)
```

#### Emperical CI coverage

##### Bias-corrected bootstrap based CI

Same procedure as model 1:

```{r "percentile-CI-model-3-parallel", results = 'hide'}
# Detect and start the workers
P <- detectCores(logical = FALSE) # physical cores
cl <- makeCluster(P)

# Initialize them with the libraries
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(boot))

# Run in parallel and append results
boot_par_results <- clusterApply(cl, 1:nsim, fun = BootParCI, datLoc = datLoc, SCEN = SCEN)
boot_CI_coverage <- do.call(rbind, boot_par_results)

# Stop the workers
stopCluster(cl)
```

```{r "table-EC-bootstrap-model-3", echo = FALSE}
resTable_tmp <- boot_CI_coverage %>% ungroup() %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(coverage), 3))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```

> Somewhat better.
