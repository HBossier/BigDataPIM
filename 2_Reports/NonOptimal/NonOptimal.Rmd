---
title: "Non Optimal Subsampling"
author: "Han Boissier"
date: "28 april 2017"
output:
  html_document:
    theme: journal
    toc: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	comment = NA,
	cache = TRUE,
	warning = FALSE
)
```

```{r "global-options", echo = FALSE, cache.rebuild = TRUE}
# libraries
library(ggplot2)
library(dplyr)
library(ggthemes)
library(RColorBrewer)
library(boot)
library(parallel)

# Custom QQ plot function
gg_qq <- function(x, distribution = "norm", ..., line.estimate = NULL, conf = 0.95,
                  labels = names(x), title = NULL){
  q.function <- eval(parse(text = paste0("q", distribution)))
  d.function <- eval(parse(text = paste0("d", distribution)))
  x <- na.omit(x)
  ord <- order(x)
  n <- length(x)
  P <- ppoints(length(x))
  df <- data.frame(ord.x = x[ord], z = q.function(P, ...))

  if(is.null(line.estimate)){
    Q.x <- quantile(df$ord.x, c(0.25, 0.75))
    Q.z <- q.function(c(0.25, 0.75), ...)
    b <- diff(Q.x)/diff(Q.z)
    coef <- c(Q.x[1] - b * Q.z[1], b)
  } else {
    coef <- coef(line.estimate(ord.x ~ z))
  }

  zz <- qnorm(1 - (1 - conf)/2)
  SE <- (coef[2]/d.function(df$z)) * sqrt(P * (1 - P)/n)
  fit.value <- coef[1] + coef[2] * df$z
  df$upper <- fit.value + zz * SE
  df$lower <- fit.value - zz * SE

  if(!is.null(labels)){ 
    df$label <- ifelse(df$ord.x > df$upper | df$ord.x < df$lower, labels[ord],"")
    }

  p <- ggplot(df, aes(x=z, y=ord.x)) +
    geom_point() + 
    geom_abline(intercept = coef[1], slope = coef[2]) +
    geom_ribbon(aes(ymin = lower, ymax = upper), alpha=0.2) + 
    scale_x_continuous(name = 'Normal Quantiles') + 
    scale_y_continuous(name = 'Sample Quantiles') +
      ggtitle("QQ-plot", subtitle = title)
  if(!is.null(labels)) p <- p + geom_text( aes(label = label))
  #print(p)
  return(p)
}

# Custom function to calculate CI coverage
checkCoverage <- function(estimate, variance, TrueValue, level = 0.95){
  CIlow <- estimate - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(variance))
  CIup <- estimate + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(variance))
  return(ifelse(TrueValue >= CIlow && TrueValue <= CIup, 1, 0))
}

# Bootstrap function
BootCI <- function(OrigData, type = 'bca'){
  # Function to calculate mean statistic
  meanBoot <- function(data, indices){
    d <- data[indices]
    return(mean(d))
  }
  # Bootstrap and CI
  boot_result <- boot(data = OrigData, statistic = meanBoot, R = 1000)
  boot_ci <- boot.ci(boot_result, type = type)
  return(data.frame('lwrBoot' = boot_ci$bca[4], 'uprboot' = boot_ci$bca[5]))
}

# Construct a function that can be used over workers in parallel
BootParCI <- function(sID, datLoc, SCEN){
  # Bootstrap function
  BootCI <- function(OrigData, type = 'bca'){
    # Function to calculate mean statistic
    meanBoot <- function(data, indices){
      d <- data[indices]
      return(mean(d))
    }
    # Bootstrap and CI
    boot_result <- boot(data = OrigData, statistic = meanBoot, R = 1000)
    boot_ci <- boot.ci(boot_result, type = type)
    return(data.frame('lwrBoot' = boot_ci$bca[4], 'uprboot' = boot_ci$bca[5]))
  }
  
  # Names of the columns in data files, according to model
  S1_colnames <- c("beta", "sVariance", "K", "nRSloops", "TrueBeta")
  S3_colnames <- c("beta.X_smartph_hrs", "beta.X_sex", 
                 "beta.X_econArea",  "beta.X_ethnic", 
                 "sVariance.X_smartph_hrs", "sVariance.X_sex",
                 "sVariance.X_econArea", 
                 "sVariance.X_ethnic",  "K", "nRSloops", "TrueBeta")
  if(SCEN == 1){
    assign(x = 'generic_colnames', value = S1_colnames)
  }
  if(SCEN == 3){
    assign(x = 'generic_colnames', value = S3_colnames)
  }
 
  # Read values from one simulation
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', sID, '_SCEN_', SCEN, '.txt'),
                 col.names = generic_colnames, header = TRUE) %>% 
                   tbl_df()
  # Rename the beta.X_smartph_hrs column to beta
  if(SCEN == 3){
    ValSim <- rename(ValSim, beta = beta.X_smartph_hrs)
  }
  
  # Run bootstrap, construct CI, append the true value, then check whether this value is in CI
  PercCI <- ValSim %>% group_by(K, nRSloops) %>% 
    do(bind_cols(BootCI(OrigData = .$beta))) %>%
    mutate(TrueBeta = as.numeric(slice(select(ValSim, TrueBeta), 1))) %>%
    do(slice(., 1)) %>% rowwise() %>% mutate(coverage = ifelse(TrueBeta >= lwrBoot && TrueBeta <= uprboot, 1, 0)) %>% 
      mutate(sim = sID) 
  return(PercCI)
}

# Get info on machine we are compiling
OS <- Sys.info()['machine']
if(OS == 'x86_64'){
  MACHINE <- "MAC"
  wd <- '/Users/hanbossier/Dropbox/Mastat/Thesis/BigDataPIM'
}else{
  MACHINE <- "DOS"
  wd <- 'D:/Users/Han/Dropbox/Mastat/Thesis/BigDataPIM'
}

# location of data
if(MACHINE == "MAC"){
datLoc <- "/Volumes/1_5_TB_Han_HDD/Mastat/Thesis/BigDataPIM/NonOptimal/"  
}
if(MACHINE == "DOS"){
datLoc <- ""  
}
```


## Introduction
In this report, we consider a non-optimal subsampling approach when fitting a PIM on a large dataset. We first simulate a dataset (sample size, $N = 250.000$) under the normal linear model. We consider several models in which we increase the complexity. Then we apply a non-optimal subsampling algorithm. We record the estimated $\beta$ parameters and the time needed (on the HPC infastructure) to estimate these. In the first section, we briefly explain the algorithm. Next we give the simlation details. Finally, we look at the results.  

## Non Optimal Subsampling
The main problem when fitting a Probabilistic Index Model (PIM) on a large dataset is the time needed to estimate the parameters. In order to limit the computational time, we first explore a non-optimal subsampling approach. Consider a set of subsampling probabilites $\pi_i, i = 1,...,n$ assigned to all data points. We define $\boldsymbol{\pi} = \{\pi_i\}_{i=1}^n$. Next we apply the following algorithm:

* **1) sample:** draw a random sample $K < N$ from the original dataset with probability $\boldsymbol{\pi}$
* **2) estimate:** estimate the $\beta$ parameters using a PIM on the subset of the data. 
* **3) iterate:** repeat step 1 for **B** times.

In this report, we consider the nonuniform subsampling probability $\boldsymbol{\pi}^{\text{UNI}} = \{\pi_i = n^{-1}\}_{i=1}^n$ in step 1. 


## Simulations

### Model 1

#### Data generation

```{r echo = FALSE}
u <- 1
alpha <- 5
sigma <- 1
trueBeta <- alpha/(sqrt(2) * sigma)
```

We first generate data under the linear model:

$$
Y_i = \alpha X_i + \varepsilon_i 
$$
with $i = 1,...,n$ and $\varepsilon_i$ are i.i.d. $N({0,\sigma^2)}$ with $\sigma^2 = 1$. We consider a sample size $n = 250.000$. The predictor (*X*) is uniformly spaced between $[0.1,1]$. The value $\alpha$ is set to 5. Using the probit link function, the corresponding PIM can be expressed as:

$$
\Phi^{-1}[P(Y\preceq Y'|X,X')] = \beta(X' - X)
$$
where $\beta = \alpha/\sqrt{2\sigma^2}$. Hence the true value for $\beta$ equals ```r 5/(sqrt(2 * 1))```.


### Model 2
For the second model, we choose $\alpha = 10$, a predictor (*X*) uniformly spaced between $[0.1,10]$ and $\sigma^2 = 25$. Hence, the true value for $\beta =$ ```r 10/(sqrt(2 * 25))```.

### Model 3
The third model is a multiple regression model in which we take parameters from the analysis done in Przybylski and Weinstein (2017). In their study, the authors used linear regression to model the effect of (among other variables) smartphone usage in the weekdays and weekends on self-reported mental well being. To control for confounding effects, they included variables for sex, whether the participant was located in economic deprived areas and the ethnic background (minority group yes/no) in the model. The full model is given as:

$$
Y_i = \mu + \alpha_1 X_i + \gamma_1 Z_{1i} + \gamma_2 Z_{2i} + \gamma_3 Z_{31i} + \varepsilon_i 
$$

Based on a linear regression, we find the regression parameters, the proportions of the covariates ($Z_j$), the range of the predictor ($X$, smartphone usage during weekdays measured on a Likert scale from 0-7) and the spread of the observations ($\sigma = 9.51$). These parameters are then used to generate normally i.i.d. data. 


```{r "dataWB", collapse=TRUE, cache = TRUE}
dataWB <- read.csv(file = paste(wd, '/OSF_Przybylski2017/data.csv', sep = ''))

# Smartphone screen usage during weekdays, corrected for sex, economic area and minority status
dataWB %>% select(mwbi, sp_wd, male, minority, deprived) %>% 
  filter(complete.cases(.)) %>%
  mutate(sp_wd_sq = sp_wd**2) %>% 
  lm(mwbi ~ sp_wd + male + minority + deprived, data = .) %>%
  summary(.)

# Spread of data
sd(dataWB$mwbi, na.rm = TRUE)

# proportion of gender, minority and deprived
mean(dataWB$male)
mean(dataWB$deprived)
mean(dataWB$minority)
```

Then for each simulation, we generate a dataset through:
```{r "example-model-3"}
# Sample size
n <- 250000
# Regression parameters
alpha_0 <- 46.72
alpha_1 <- -0.43
# Control variables: sex (0 = female, 1 = male)
sex <- c(0,1)
  alpha_sex <- 4.55
# Economic area (deprived = 1, otherwise 0)
econArea <- c(0,1)
  alphaEcon <- -0.45
# Ethnic background (white = 0, otherwise 1)
ethnic <- c(0,1)
  alpha_ethnic <- 0.30
# Predictor is a discrete scale from 0 --> 7
u <- 7
# Sigma based on dataset
sigma <- 9.51

# Seed: note, in simulations, this seed depends on ID of simulation!
set.seed(123)

# Predictors: proportions come from observed proportions in study of mental well being
X_smartph_hrs <- sample(x = c(0:u), size = n, replace = TRUE)
X_sex <- sample(x = sex, size = n, replace = TRUE, prob = c(1-0.4758, 0.4758))
X_econArea <- sample(x = econArea, size = n, replace = TRUE, prob = c(1-0.4348, 0.4348))
X_ethnic <- sample(x = ethnic, size = n, replace = TRUE, prob = c(1-0.2408, 0.2408))

 # Observed data
Y <- alpha_0 + alpha_1*X_smartph_hrs + alpha_sex*X_sex +
       alphaEcon*X_econArea + alpha_ethnic*X_ethnic +
        rnorm(n = n, mean = 0, sd = sigma)
```
The true value for $\beta =$ ```r -0.43/(sqrt(2) * 9.51)```.

For comparisson, we plot the distribution of mental well being in the dataset (left) and $n = 250.000$ simulated datapoints (right). 

```{r "histograms", fig.width=10, fig.align="center"}
par(mfrow = c(1,2), mar = c(4,2,1,1))
hist(dataWB$mwbi, main = "Przybylski and Weinstein (2017)", xlab = "Mental well-being")
hist(Y, main = "Simulated data set", xlab = "Mental well-being")
```

```{r echo = FALSE}
par(mfrow = c(1,1))
```

> Note that we did not introduce the observed skeweness in the real dataset. 


### Model 4
For the fourth model, we choose $\alpha = 1$, a predictor (*X*) uniformly spaced between $[0.1,10]$ and $\sigma^2 = 25$. Hence, the true value for $\beta =$ ```r 1/(sqrt(2 * 25))```.


### Simulation parameters
We vary the amount of iterations **B** as well as the number of selected samples **K** in each iteration to find the optimal balance in bias and computation time. Both **B** and **K** range from 10 to 1000. We consider 10 levels between these points for each parameter. Hence we have $10 \times 10 = 100$ scenarios. 

```{r "simulation parameters", echo = FALSE}
# number of simulations 
nsim <- 1000

# Confidence level (for CI section)
level <- 0.95

# Sample size
n <- 250000

# Vector of number of resampling loops
nRSloops_vec <- floor(seq(10,1000,length.out = 10))

# Vector of number of selected datapoints K per iteraton 
K_vec <- floor(seq(10,1000,length.out = 10))

# Number of pairs/combinations between number of resampling loops and sampled data 
pairs <- expand.grid(B = nRSloops_vec, K = K_vec)
cbind(head(pairs),' ...' = c(' ...',' ...',' ...',' ...',' ...',' ...'),tail(pairs))
```

We simulate each combination of **B** and **K** for 1000 times.

## Results

### Model 1

Parameters:
```{r "model-1-parameters", cache.rebuild = TRUE}
u <- 1
alpha <- 5
sigma <- 1
trueBeta <- alpha/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 1
```

#### Load in data

Start with estimated $\beta$:
```{r echo = FALSE}
# Data frame with all values over all simulations
valuesAllSim <- timeAllSim <- data.frame() %>% tbl_df()

# Progress
progress <- floor(seq(1,nsim,length.out = 11)[-1])
```


```{r "load-beta-model-1", results = 'hide', cache = TRUE}
# load in estimated beta values
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
               col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), 
               header = TRUE) %>% tbl_df() %>%
      group_by(K, nRSloops, TrueBeta) %>% summarise(avBeta = mean(beta)) %>% 
      mutate(sim = i), silent = TRUE)
  if(class(ValSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  valuesAllSim <- bind_rows(valuesAllSim, ValSim)
}
```

Same for computational time:
```{r "load-time-model-1", results = 'hide', cache = TRUE}
# load in computational time
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: computational time 
  TimeSim <-  try(read.table(file = paste0(datLoc, 'uni_time_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
              col.names = c("minutes"), header = TRUE) %>% tbl_df() %>%
              bind_cols(., pairs) %>% 
              mutate(sim = i), silent = TRUE)
    if(class(TimeSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  timeAllSim <- bind_rows(timeAllSim, TimeSim)
}
```

#### Bias vs computational time

Quick summary:
```{r collapse = TRUE}
tbl_df(valuesAllSim)
group_by(valuesAllSim, K, nRSloops) %>% summarise(avBeta = mean(avBeta)) %>% summary()
```

Plotting the estimated $\beta$ parameters for each simulation through facets on the number of iterations (**B**).

```{r "non-evaluated-plot-model-1", eval = FALSE, echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = factor(K), y = avBeta, group = factor(nRSloops))) + 
  geom_jitter(aes(colour = factor(nRSloops)), height = 0) +
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_y_continuous(name = expression(hat(beta))) +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Estimated beta") +
  geom_hline(yintercept = trueBeta, size = 1) + 
  theme_minimal()
```


```{r "beta-model-1", echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = K, y = avBeta)) + 
  geom_point(size = 0.7, colour = "#67a9cf") + 
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = expression(hat(beta))) +
  geom_line(data = mutate(.data = valuesAllSim, TrueValue = TrueBeta, Label = "True Value of Beta"),
             aes(y = TrueValue, linetype = Label), size = 0.7, show.legend = TRUE) +
  scale_linetype_discrete(name = "") +
  ggtitle("Estimated beta parameter given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

```{r "calculate-MSE-model-1"}
MSEtable_tmp <- group_by(valuesAllSim, K, nRSloops) %>% mutate(SE = (avBeta - TrueBeta)^2) %>%
  summarise(MSE = mean(SE, na.rm = TRUE))
MSEtable <- matrix(MSEtable_tmp$MSE, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(MSEtable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(MSEtable) <- paste('K = ', K_vec, sep = '')
options( scipen = 6 )
knitr::kable(MSEtable)
```

Plotting the computational time.

```{r "time-model-1", echo=FALSE, fig.align="center"}
TimeToPlot <- timeAllSim %>% group_by(B,K) %>% summarise(AvgMin = mean(minutes))
  TimeToPlot$B <- factor(TimeToPlot$B)
  TimeToPlot$K <- factor(TimeToPlot$K)

ggplot(data = TimeToPlot, aes(x = K, y = AvgMin, group = B)) + 
  geom_line(aes(colour = B),size = 1) +
  scale_y_continuous(name = "Minutes",
                     limits = c(0,max(TimeToPlot$AvgMin) + 2)) + 
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Average computational time to estimate PIM on HPC") +
  theme_minimal()

```

> Note: this is run on the HPC which will runs faster than regular PCs.

#### Combine bias and time
Can we combine time and bias in one graph? Maybe create a summary measure which involves a penalty term for the required time to estimate the parameters. 

#### Distribution of estimator
The figures below show the QQ-plots of $\widehat\beta$ associated with model 1 when having 500 samples in one iteration and applying 200 iterations.

```{r "not-evaluated-QQ-plot-model-1", eval = FALSE, echo = FALSE}
valuesAllSim %>% filter(K == 340 & nRSloops == 230) %>% ggplot(aes(sample =
avBeta)) + stat_qq() + ggtitle('Normal QQ-plot when K = 340 and B = 230')

valuesAllSim %>% filter(K == 230 & nRSloops == 120) %>% ggplot(aes(sample =
avBeta)) + stat_qq()
```


```{r "QQplot-model-1", fig.show='hold', fig.width=10}
for(j in 1:length(nRSloops_vec)){
  K_temp <- K_vec[j]
  B_temp <- nRSloops_vec[j]
  assign(paste0("Plot", j),
    valuesAllSim %>% filter(K == K_temp & nRSloops == B_temp) %>% 
    select(avBeta) %>% unlist(.) %>% 
    as.numeric() %>% gg_qq(x = ., title = paste0('K = ', K_temp, ' and B = ', B_temp))
  )
}

cowplot::plot_grid(Plot1, Plot2, Plot3, Plot4, Plot5, Plot6, labels = c("A", "B", "C", "D", "E", "F"), ncol = 3)
cowplot::plot_grid(Plot7, Plot8, Plot9, Plot10, labels = c("G", "H", "I", "J"), ncol = 3)
```


#### Emperical CI coverage

##### Within iteration sandwich estimator

In this section, we calculate for every iteration (**B**) the 95% confidence interval. Within each iteration, we can check its nominal coverage level and then average over all simulations. Note, that this is just equal to running multiple simulations in which you check the asymptotic properties of the sandwich estimator and not related to our research question! 

```{r calculate-CI-model-1}
# Empty data frame
valuesAvgCov <- data.frame() %>% tbl_df()

# load in estimated beta values and calculate coverage
for(i in 1:nsim){
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
                 col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                   tbl_df(), silent = TRUE)
  if(class(ValSim)[1] == "try-error"){
    print(i);next
  }
  # Calculate coverage per simulation: estimate +/- (1.96 * sqrt(sandwichVariance)) for 95% CI
  WithCoverage <- mutate(ValSim, CIlow = beta - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>%
              mutate(CIup = beta + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>% rowwise() %>%
              mutate(coverage = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0))
      
  # Average coverage per simulation
  AvgCov <- WithCoverage %>% ungroup() %>% group_by(K, nRSloops) %>%
      summarise(AvgCov = mean(coverage, na.rm = TRUE)) %>% 
      mutate(sim = i)
  
  # Add to data frame with all simulations
  valuesAvgCov <- bind_rows(valuesAvgCov, AvgCov)
}
```

Now we proceed to plot the average CI level per **B** and **K**.

```{r "plot-CI-model-1", echo = FALSE, fig.align = 'center'}
ggplot(valuesAvgCov, aes(x = K, y = AvgCov)) + 
  geom_point(size = 0.7, colour = "#67a9cf") +
  geom_hline(aes(yintercept = level), size = 0.7) +
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = "95% CI coverage") +
  scale_linetype_discrete(name = "") +
  ggtitle("Average CI coverage given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

For more detail, we refer to the table below with the average emperical coverage per **B** (columns) and **K** (rows).
```{r "table-EC-sandwich-model-1", echo = FALSE}
resTable_tmp <- valuesAvgCov %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(AvgCov), 3))
resSDtable <- valuesAvgCov %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(AvgCov), 3), SD = round(sd(AvgCov), 2))
#knitr::kable(valuesAvgCov %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(AvgCov), 2), SD = round(sd(AvgCov), 2)))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```


##### Bias-corrected bootstrap based CI

Now we turn to the more interesting question. How can we create confidence intervals around the average $\beta$ PIM estimate obtained in one iteration in the non optimal resampling procedure? We suggest to use a bootstrap procedure. Let us try with the bias-corrected and accelerated bootstrap (BCa) which adjusts both for bias and skeweness. Note, this might introduce extra computation time. To get the results here, we run all simulations in parallel.
Note that we defined the function to calculate the coverages (*BootParCI*) earlier.

```{r "percentile-CI-model-1", eval = FALSE, echo = FALSE}
# Empty data frame
valuePercCI <- data.frame() %>% tbl_df()

# load in estimated beta values and calculate coverage
for(i in 1:nsim){
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <- try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
                 col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                   tbl_df(), silent = TRUE)
  if(class(ValSim)[1] == "try-error"){
    print(i);next
  }
  
  # Create the percentile based CI lower and upper bound, 
    # then take one value for each combination of B and K
    # then check whether true value is between bounds
  PercCI <- ValSim %>% group_by(K, nRSloops) %>% 
    do(mutate(.,PercLower = quantile(x = .$beta, probs = c((1-level)/2)),
                PercUpper = quantile(x = .$beta, probs = c(1-(1-level)/2)))) %>%
    do(slice(., 1)) %>% rowwise() %>% mutate(coverage = ifelse(TrueBeta >= PercLower && TrueBeta <= PercUpper, 1, 0)) %>% 
      mutate(sim = i) 
  
  # Add to data frame with all simulations
  valuePercCI <- bind_rows(valuePercCI, PercCI)
}
```


```{r "percentile-CI-model-1-sequential", eval = FALSE, echo = FALSE}
# Empty data frame
valuePercCI <- data.frame() %>% tbl_df()

# load in estimated beta values and calculate coverage
for(i in 1:nsim){
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <- try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
                 col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                   tbl_df(), silent = TRUE)
  if(class(ValSim)[1] == "try-error"){
    print(i);next
  }
  
  # Run bootstrap, construct CI, append the true value, then check whether this value is in CI
  PercCI <- ValSim %>% group_by(K, nRSloops) %>% 
    do(bind_cols(BootCI(OrigData = .$beta))) %>%
    mutate(TrueBeta = as.numeric(slice(select(ValSim, TrueBeta), 1))) %>%
    do(slice(., 1)) %>% rowwise() %>% mutate(coverage = ifelse(TrueBeta >= lwrBoot && TrueBeta <= uprboot, 1, 0)) %>% 
      mutate(sim = i) 
  
  # Add to data frame with all simulations
  valuePercCI <- bind_rows(valuePercCI, PercCI)
}
```


```{r "percentile-CI-model-1-parallel", cache = TRUE, results = 'hide'}
# Detect and start the workers
P <- detectCores(logical = FALSE) # physical cores
cl <- makeCluster(P)

# Initialize them with the libraries
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(boot))

# Run in parallel and append results
boot_par_results <- clusterApply(cl, 1:nsim, fun = BootParCI, datLoc = datLoc, SCEN = SCEN)
boot_CI_coverage <- do.call(rbind, boot_par_results)

# Stop the workers
stopCluster(cl)
```

```{r "table-EC-bootstrap-model-1", echo = FALSE, cache.rebuild = TRUE}
resTable_tmp <- boot_CI_coverage %>% ungroup() %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(coverage), 3))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```

> Quite bad coverages!


```{r "test-code", eval = FALSE, echo = FALSE}
# Bootstrap function
BootCI <- function(OrigData, type = 'bca'){
  # Function to calculate mean statistic
  meanBoot <- function(data, indices){
    d <- data[indices]
    return(mean(d))
  }
  # Bootstrap and CI
  boot_result <- boot(data = OrigData, statistic = meanBoot, R = 1000)
  boot_ci <- boot.ci(boot_result, type = type)
  return(data.frame('lwrBoot' = boot_ci$bca[4], 'uprboot' = boot_ci$bca[5]))
}
str(boot_result)
mean(boot_result$t)
boot_ci$bca[5] - boot_result$t0
boot_result$t0 - boot_ci$bca[4]

boot_ci$bca[5] - mean(boot_result$t)
mean(boot_result$t) - boot_ci$bca[4]

  #View(valuePercCI)
  table(valuePercCI$coverage)

ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', sample(x = 1:1000, size = 1), '_SCEN_', SCEN, '.txt'),
               col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
                 tbl_df()
test <- ValSim %>% group_by(K, nRSloops) %>% filter(K == 230 & nRSloops == 230)
BootResults <- boot(data = test$beta, statistic = meanBoot, R = 1000)
boot.ci(BootResults, type = 'bca')
TrueBeta <- 3.53534

boot.ci(BootResults, type = 'bca')$bca
BootResults
BootResults$t %>% sd()
mean(BootResults$t) - BootResults$t0


str(boot.ci(BootResults, type = 'bca'))
unlist(boot.ci(BootResults, type = 'bca'))[c('bca4', 'bca5')]
boot.ci(BootResults, type = 'bca')$bca[4]

alpha_1 <- -0.43
# Sigma based on dataset
sigma <- 9.51
trueBeta <- alpha_1/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 3
S3_colnames <- c("beta.X_smartph_hrs", "beta.X_sex", 
                 "beta.X_econArea",  "beta.X_ethnic", 
                 "sVariance.X_smartph_hrs", "sVariance.X_sex",
                 "sVariance.X_econArea", 
                 "sVariance.X_ethnic",  "K", "nRSloops", "TrueBeta")

TrueBeta <- 3.53534
TrueBeta <- -0.03197223

bootVector <- sandwichVector <- sandwichIndVector <- 
    lowBoundVarVector <- SEbootVector <- SEVector <- upBoundVarVector <- BothAvgVector <- c()
for(i in 1:500){
  if(i == 250) print('25%')
  if(i == 500) print('50%')
  if(i == 750) print('75%')
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = S3_colnames, header = TRUE) %>% 
           tbl_df()
  test <- ValSim %>% group_by(K, nRSloops) %>% filter(K == 230 & nRSloops == 230)
  test <- rename(test, beta = beta.X_smartph_hrs, sVariance = sVariance.X_smartph_hrs)

  #   # Bootstrap
  # BootResults <- boot(data = test$beta, statistic = meanBoot, R = 1000)
  # boot_ci <- boot.ci(BootResults, type = 'norm')
  # bootVector <- c(bootVector, ifelse(TrueBeta >= boot_ci$norm[2] & TrueBeta <= boot_ci$norm[3], 1, 0))
  # 
  # # SE of bootstrap
  # seBootLow <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(BootResults$t))
  # seBootUp <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(BootResults$t))
  # SEbootVector <- c(SEbootVector, ifelse(TrueBeta >= seBootLow & TrueBeta <= seBootUp, 1, 0))
  # 
  # # SE through sd of resamples
  # seLow <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(test$beta))
  # seUp <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sd(test$beta))
  # SEVector <- c(SEVector, ifelse(TrueBeta >= seLow & TrueBeta <= seUp, 1, 0))
  # 
  # # Mean of sandwich estimated variance
  # sLow <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(mean(test$sVariance)))
  # sUp <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(mean(test$sVariance)))
  # sandwichVector <- c(sandwichVector, ifelse(TrueBeta >= sLow & TrueBeta <= sUp, 1, 0))
  # 
  # # Mean of CI bounds
  # WithCI <- mutate(test, CIlow = beta - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>%
  #             mutate(CIup = beta + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance)))
  # sandwichIndVector <- c(sandwichIndVector, ifelse(TrueBeta >= mean(WithCI$CIlow) & TrueBeta <= mean(WithCI$CIup), 1, 0))
  # 
  # # Lower bound of bootstrap on sandwich variance
  # varBoot <- boot(data = test$sVariance, statistic = meanBoot, R = 1000)
  # lowerbound <- boot.ci(varBoot, type = 'norm')$norm[2]
  # lowBoundVar <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(lowerbound))
  # upBoundVar <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(lowerbound))
  # lowBoundVarVector <- c(lowBoundVarVector, ifelse(TrueBeta >= lowBoundVar & TrueBeta <= upBoundVar, 1, 0))
  # 
  # # Upper bound of bootstrap on sandwich variance
  # upperbound <- boot.ci(varBoot, type = 'norm')$norm[3]
  # lowUPBoundVar <- mean(test$beta) - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(upperbound))
  # upUPBoundVar <- mean(test$beta) + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(upperbound))
  # upBoundVarVector <- c(upBoundVarVector, ifelse(TrueBeta >= lowUPBoundVar & TrueBeta <= upUPBoundVar, 1, 0))
  # 
  # Calculate CI on each datapoint, using sandwich variance
  BothAvg_tmp <- mutate(test,  sVarLow = beta - (qnorm(0.025, lower.tail = FALSE) * sqrt(sVariance)), 
                sVarUp = beta + (qnorm(0.025, lower.tail = FALSE) * sqrt(sVariance)))
  BothAvg <- summarise(BothAvg_tmp, AvgLow = mean(sVarLow, na.rm = TRUE), AvgUp = mean(sVarUp, na.rm = TRUE))
  BothAvgVector <- c(BothAvgVector, ifelse(TrueBeta >= BothAvg$AvgLow && TrueBeta <= BothAvg$AvgUp, 1, 0))
}

mean(bootVector)
mean(SEbootVector)
mean(SEVector)
mean(sandwichVector)
mean(sandwichIndVector)
mean(lowBoundVarVector)
mean(upBoundVarVector)
mean(BothAvgVector)

avgBeta <- avgVariance <- c()
TrueBeta <- -0.03197223
# Check distribution of beta 
san_BothAvgVector <- norm_BothAvg <- norm_tAvg <- c()
for(i in 1:100){
  if(i == 250) print('25%')
  if(i == 500) print('50%')
  if(i == 750) print('75%')
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = S3_colnames, header = TRUE) %>% 
           tbl_df() %>% select(-c(beta.X_sex, beta.X_econArea, beta.X_ethnic,sVariance.X_sex, sVariance.X_econArea, sVariance.X_ethnic))
  DataSim <- ValSim %>% group_by(K, nRSloops) %>% filter(K == 230 & nRSloops == 1000) %>%
      rename(beta = beta.X_smartph_hrs, sVariance = sVariance.X_smartph_hrs)
  
  # Average beta in vector
  avgBeta <- c(avgBeta, summarise(DataSim, AvgBeta = mean(beta, na.rm = TRUE))$AvgBeta)
  
  # Average variance estimator
  avgVariance <- c(avgVariance, summarise(DataSim, AvgVar = mean(sVariance, na.rm = TRUE))$AvgVar)

  # Calculate CI on each datapoint, using sandwich variance
  san_BothAvg_tmp <- mutate(DataSim,  sVarLow = beta - (qnorm(0.025, lower.tail = FALSE) * sqrt(sVariance)),
                sVarUp = beta + (qnorm(0.025, lower.tail = FALSE) * sqrt(sVariance)))
    # Now construct one CI using means of both bounds
    san_BothAvg <- summarise(san_BothAvg_tmp, AvgLow = mean(sVarLow, na.rm = TRUE), AvgUp = mean(sVarUp, na.rm = TRUE))
    san_BothAvgVector <- c(san_BothAvgVector, ifelse(TrueBeta >= san_BothAvg$AvgLow && TrueBeta <= san_BothAvg$AvgUp, 1, 0))
    
  # CI using SD of data and normal quantiles and using t-quantiles
      # Formula = sd/sqrt(n) for some strange reason (I expected sd of beta being equal to se)
  norm_tmp <- mutate(DataSim,  
                    CIlow = beta - (qnorm(0.025, lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
                    CIup = beta + (qnorm(0.025, lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
                    type = 'norm') %>% group_by(K, TrueBeta, type) %>%
              summarise(AvgBeta = mean(beta, na.rm = TRUE),
                    AvgLow = mean(CIlow, na.rm = TRUE), 
                    AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup() %>%
              mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0),
                    sim = i)
  t_tmp <- mutate(DataSim,
                    CIlow = beta - (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
                    CIup = beta + (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
                    type = 't') %>% group_by(K, TrueBeta, type) %>%
              summarise(AvgBeta = mean(beta, na.rm = TRUE),
                    AvgLow = mean(CIlow, na.rm = TRUE), 
                    AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup() %>%
              mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0),
                    sim = i)
  
  # CI using mean of sandwich variance
  avgSvar_tmp <- mutate(DataSim,
                    CIlow = beta - (qnorm(0.025, lower.tail = FALSE) * (sqrt(mean(sVariance))/(sqrt(K/250000)))),
                    CIup = beta + (qnorm(0.025, lower.tail = FALSE) * (sqrt(mean(sVariance))/(sqrt(K/250000)))),
                    type = 'mean_sVar') %>% group_by(K, TrueBeta, type) %>%
              summarise(AvgBeta = mean(beta, na.rm = TRUE),
                    AvgLow = mean(CIlow, na.rm = TRUE), 
                    AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup() %>%
              mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0),
                    sim = i)
  
    # Again, construct CI by averaging endpoints
    norm_tAvg <- bind_rows(norm_tAvg,norm_tmp, t_tmp, avgSvar_tmp)
}

# Plot
norm_tAvg %>% 
  ggplot(., aes(x = trueBeta)) + geom_vline(xintercept = trueBeta) +
  geom_point(aes(x = AvgBeta, y = sim)) + 
  geom_segment(aes(x = AvgLow, xend = AvgUp, y = sim, yend = sim)) + 
  facet_wrap( ~ type) +
  scale_x_continuous("beta") + scale_y_continuous("simulation") +
  ggtitle("95% CI around beta PIM estimate")

# EC
group_by(norm_tAvg, type) %>% summarise(EC = mean(coverage_ind))

# Plot of individual sandwich variances
san_BothAvg_tmp %>% mutate(subject = seq(1:dim(san_BothAvg_tmp)[1])) %>% slice(1:100) %>%
  ggplot(., aes(x = trueBeta, y = subject)) + geom_vline(xintercept = trueBeta) +
  geom_point(aes(x = beta)) + 
  geom_segment(aes(x = sVarUp, xend = sVarLow, y = subject, yend = subject)) 

# Average of sandwich variance
norm_tAvg %>% 
  ggplot(., aes(x = trueBeta)) + geom_vline(xintercept = trueBeta) +
  geom_point(aes(x = AvgBeta, y = sim)) + 
  geom_segment(aes(x = AvgLow, xend = AvgUp, y = sim, yend = sim)) + 
  facet_wrap( ~ type) +
  scale_x_continuous("beta") + scale_y_continuous("simulation") +
  ggtitle("95% CI around beta PIM estimate")

avBeta <- c()
for(i in 1:250){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
           tbl_df()
  test <- ValSim %>% group_by(K, nRSloops) %>% filter(K == 340 & nRSloops == 120)
  avBeta <- c(avBeta, mean(test$beta))
}
mean(avBeta)
var(avBeta)  

  
# Check bootstrap on original dataset versus variance estimator 
# Generate predictor
X <- runif(n = 230, min = 0.1, max = 10)
 # Generate data
Y <- alpha*X + rnorm(n = 230, mean = 0, sd = sigma)
 # In data frame
OrigData <- data.frame(Y = Y, X = X)

PIMfit <- pim(formula = Y ~ X, data = OrigData,
              link = 'probit', model = 'difference')
PIMfit@vcov
bootPIM <- function(data, indices){
  d <- data[indices, ]
  return(pim(formula = Y ~ X, data = d,
              link = 'probit', model = 'difference')@coef)
}
bootPIM_results <- boot(data = OrigData, statistic = bootPIM, R = 1000)
boot.ci(bootPIM_results, type = "bca")
PIMfit@coef + (1.96 * sqrt(PIMfit@vcov))
PIMfit@coef - (1.96 * sqrt(PIMfit@vcov))

bootLM <-  function(data, indices){
  d <- data[indices, ]
  return(lm(Y ~ X, data = d)$coef[2])
}
summary(lm(Y ~ X, data = OrigData))
bootLM_results <- boot(data = OrigData, statistic = bootLM, R = 1000)
confint(lm(Y ~ X, data = OrigData), level = 0.95)
boot.ci(bootLM_results, type = "bca")


X <- runif(n = 1000000, min = 0.1, max = 10)
 # Generate data
Y <- alpha*X + rnorm(n = 1000000, mean = 0, sd = sigma)
 # In data frame
OrigData <- data.frame(Y = Y, X = X)
summary(lm(Y ~ X, data = OrigData))


mean.fun <- function(d, i) 
{    m <- mean(d$hours[i])
     n <- length(i)
     v <- (n-1)*var(d$hours[i])/n^2
     c(m, v)
}
air.boot <- boot(aircondit, mean.fun, R = 999)
boot.ci(air.boot, type = c("norm", "basic", "perc", "stud"))

mean.fun <- function(d, i) 
{    m <- mean(d$hours[i])
     n <- length(i)
     v <- (n-1)*var(d$hours[i])/n^2
     c(m)
}
air.boot <- boot(aircondit, mean.fun, R = 999)



WithCoverage <- mutate(test, CIlow = beta - (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>%
              mutate(CIup = beta + (qnorm(p = ((1 - level)/2), lower.tail = FALSE) * sqrt(sVariance))) %>% rowwise() %>%
              mutate(coverage = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0))
WithCoverage$coverage %>% mean()
mean(test$beta)
mean(test$sVariance)
BootResults$t %>% sd()
sqrt(0.05561626)
BootCI(test$beta)
OrigData <- test$beta

ValSim %>% group_by(K, nRSloops) %>% 
    do(mutate_(.,BootCI(OrigData = .$beta)))

t1 <- Sys.time()
ValSim %>% filter(K == 230 & nRSloops == 230) %>% group_by(K, nRSloops) %>% 
  do(bind_cols(BootCI(OrigData = .$beta)))
Sys.time() - t1

t1 <- Sys.time()
ValSim %>% group_by(K, nRSloops) %>% 
  do(bind_cols(BootCI(OrigData = .$beta)))
Sys.time() - t1

TrueBeta_tmp <- as.numeric(slice(select(ValSim, TrueBeta), 1))
ValSim %>% filter(K == c(10,120) & nRSloops == c(10,120)) %>%  group_by(K, nRSloops) %>% 
  do(bind_cols(BootCI(OrigData = .$beta))) %>% 
  mutate(TrueBeta = TrueBeta_tmp)


  
ValSim %>% filter(K == c(10,120) & nRSloops == c(10,120)) %>% 
  group_by(K, nRSloops) %>% 
  slice_rows() %>%
  by_slice(~ BootCI(.x$beta), .collate = "rows")

  by_slice(~ describe(.x$price), .collate = "rows")
do(BootCI(OrigData = .$beta))
?by_slice

# norm_tAvg %>% filter(type == 't' & K == 230) %>% group_by(K,nRSloops)
# ifelse(3.535534 >= 3.352112 &&  3.535534 <= 3.539192, 1,0)
# 
# tes <- filter(ValSim, K == 230 & nRSloops == 230)
# test <- mutate(tes,
#           CIlow = beta - (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
#           CIup = beta + (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
#           type = 't') 
# View(test)
# ind_c <- c()
# for(j in 1:dim(test)[1]){
#   ind_c <- c(ind_c, ifelse(test[j,'TrueBeta'] >= test[j,'CIlow'] && test[j, 'TrueBeta'] <= test[j,'CIup'], 1,0))
# }
# 
# 
# %>% group_by(K, nRSloops, TrueBeta, type) %>%
#     summarise(AvgBeta = mean(beta, na.rm = TRUE),
#           AvgLow = mean(CIlow, na.rm = TRUE),
#           AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup()
# test %>% rowwise() %>% mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0)) %>% View()
#            ,
#           sim = i)
# %>% summarise(EC = mean(coverage_ind))
#   View()

# norm_tAvg %>% filter(type == 't' & K == 230) %>% group_by(K,nRSloops)
# ifelse(3.535534 >= 3.352112 &&  3.535534 <= 3.539192, 1,0)
# 
# tes <- filter(ValSim, K == 230 & nRSloops == 230)
# test <- mutate(tes,
#           CIlow = beta - (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
#           CIup = beta + (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
#           type = 't') 
# View(test)
# ind_c <- c()
# for(j in 1:dim(test)[1]){
#   ind_c <- c(ind_c, ifelse(test[j,'TrueBeta'] >= test[j,'CIlow'] && test[j, 'TrueBeta'] <= test[j,'CIup'], 1,0))
# }
# 
# 
# %>% group_by(K, nRSloops, TrueBeta, type) %>%
#     summarise(AvgBeta = mean(beta, na.rm = TRUE),
#           AvgLow = mean(CIlow, na.rm = TRUE),
#           AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup()
# test %>% rowwise() %>% mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0)) %>% View()
#            ,
#           sim = i)
# %>% summarise(EC = mean(coverage_ind))
#   View()

```

##### Student - t confidence intervals

What happens if we consider the PIM $\beta$ estimates as **K** i.i.d. datapoints and construct a $t$-distributed CI interval? Using the standard deviation of the estimated $\beta$ parameters, we get:

$$
[\bar{\beta} \pm t_{\alpha/2, K-1} sd(\beta)/\sqrt{(K - 1)}]
$$
Note, this is not really correct. But we just want to see what effect it has on the emperical coverage.


```{r "calculate-student-t-CI-model-1"}
norm_tAvg <- c()
# Read in data (again, sigh)
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
        tbl_df()
  # CI using SD of data and normal quantiles and using t-quantiles
    # Formula = sd/sqrt(n) for some strange reason (I expected sd of beta being equal to se)
  norm_tmp <- ValSim %>% group_by(K, nRSloops) %>% 
          mutate(
          CIlow = beta - (qnorm(0.025, lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          CIup = beta + (qnorm(0.025, lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          type = 'norm') %>% group_by(K, nRSloops, TrueBeta, type) %>%
    summarise(AvgBeta = mean(beta, na.rm = TRUE),
          AvgLow = mean(CIlow, na.rm = TRUE), 
          AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup() %>% rowwise() %>% 
    mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0),
          sim = i)
  t_tmp <- ValSim %>% group_by(K, nRSloops) %>% 
          mutate(
          CIlow = beta - (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          CIup = beta + (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          type = 't') %>% group_by(K, nRSloops, TrueBeta, type) %>%
    summarise(AvgBeta = mean(beta, na.rm = TRUE),
          AvgLow = mean(CIlow, na.rm = TRUE), 
          AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup() %>% rowwise() %>% 
    mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0),
          sim = i)
  
  # Construct general CI by averaging endpoints
  norm_tAvg <- bind_rows(norm_tAvg,norm_tmp, t_tmp)
}
```

```{r "visualize-student-t-CI-model-1"}
# Plot
norm_tAvg %>% filter(type == 't' & K == 1000 & nRSloops == 1000) %>% 
  slice(round(seq(1,nsim,length.out = 50),0)) %>%
   ggplot(., aes(x = TrueBeta)) + geom_vline(aes(xintercept = TrueBeta)) +
   geom_point(aes(x = AvgBeta, y = sim)) + 
   geom_segment(aes(x = AvgLow, xend = AvgUp, y = sim, yend = sim)) + 
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
   ggtitle("95% CI around beta PIM estimate")

resTable_tmp <- norm_tAvg %>% filter(type == 't') %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```


##### Bootstrap m-out-of n

Essentially, the non-optimal subsampling algorithm can also be consider as bootstrapping m-out-of n, with m $<<$ n. In our case, m corresponds to **K** the subsample size. 

However, in the bootstrap algorithm, it is essentially to scale the CI in order to match the variability of the full dataset. This scaling is done by multiplying the standard error of the estimator ($se(\hat\theta_n)$) through:

$$
se(\hat\theta_n) \approx se(\theta^*_k) \times \sqrt{\frac{K}{n}}
$$
In which $se(\theta^*_k)$ is equal to the standard deviation of the sampling distribution for each iteration **B**. 

We shall try this here.

```{r "calculate-m-out-of-n-CI-model-1", cache = TRUE}
scaledSD <- data.frame() %>% tbl_df()
# Read in data
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
        tbl_df()
  # CI using SD of beta with scaling
  sdScale_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    sdBeta = sd(beta, na.rm = TRUE)) %>%
          mutate(CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) * sdBeta * sqrt(K/n)),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * sdBeta * sqrt(K/n)),
              type = 'scaledSD') %>% 
          ungroup() %>% rowwise() %>% 
          mutate(coverage_ind = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0),
              sim = i)
  
  # Collect the values over all simulations
  scaledSD <- bind_rows(scaledSD,sdScale_tmp)
}
```

```{r "calculate-m-out-of-npairs-CI-model-1", eval = FALSE, echo = FALSE}
scaledPairSD <- data.frame() %>% tbl_df()
# Read in data
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
        tbl_df()
  # CI using SD of beta with scaling
  sdScalePair_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    sdBeta = sd(beta, na.rm = TRUE)) %>%
          mutate(CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) * 
                                       sdBeta * sqrt((K * (K-1)/2)/(n * (n-1)/2))),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * 
                                  sdBeta * sqrt((K * (K-1)/2)/(n * (n-1)/2))),
              type = 'scaledPairSD') %>% 
          ungroup() %>% rowwise() %>% 
          mutate(coverage_ind = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0),
              sim = i)
  
  # Collect the values over all simulations
  scaledPairSD <- bind_rows(scaledPairSD,sdScalePair_tmp)
}
```


```{r "plot-scaled-CI-model-1", echo = FALSE, fig.align = 'center', fig.width = 5, fig.height=6}
ToPlotCI <- scaledSD %>%  filter(K %in% c(230, 1000) & nRSloops %in% c(230, 1000)) %>% 
  group_by(K, nRSloops) %>%
  slice(seq(1, nsim, length.out = 100)) %>% ungroup()
ToPlotCI$K <- factor(ToPlotCI$K, levels = c(230, 1000), labels = c('K = 230', 'K = 1000'))
ToPlotCI$nRSloops <- factor(ToPlotCI$nRSloops, levels = c(230, 1000), labels = c('B = 230', 'B = 1000'))

ggplot(ToPlotCI, aes(x = TrueBeta)) + geom_vline(aes(xintercept = TrueBeta), size = .8, alpha = .8) +
   geom_point(aes(x = AvgBeta, y = sim, colour = factor(coverage_ind)), size = 0.5) + 
   geom_segment(aes(x = CIlow, xend = CIup, y = sim, yend = sim, colour = factor(coverage_ind), alpha = factor(coverage_ind)), size = 0.9) + 
  facet_wrap(K ~ nRSloops, dir = 'v') +
  scale_alpha_manual("Contains true value", values = c(1,0.8), labels = c("NO", "YES")) +
  scale_colour_manual("Contains true value", values = c('#d95f02', '#1b9e77'), labels = c("NO", "YES")) +
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
  theme(legend.position="bottom") +
   ggtitle("100 random selected simulations with the 95% CI")
```

```{r "table-scaled-sd-model-1"}
scaledSDTable_tmp <- scaledSD %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
scaledSDTable <- matrix(scaledSDTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(scaledSDTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(scaledSDTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(scaledSDTable)
```

##### Scaled sandwich variance

```{r "calculate-sandwich-variance-CI-model-1", cache = TRUE}
scaledSVAR <- data.frame() %>% tbl_df()
# Read in data
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
        tbl_df()
  # CI using mean sandwich variance of beta with scaling
  svarScale_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    AvgSvarBeta = mean(sVariance, na.rm = TRUE)) %>%
          mutate(CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) *
                                       sqrt(AvgSvarBeta) * sqrt(K/n)),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * 
                                  sqrt(AvgSvarBeta) * sqrt(K/n)),
              type = 'scaledSVAR') %>% 
          ungroup() %>% rowwise() %>% 
          mutate(coverage_ind = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0),
              sim = i)
  
  # Collect the values over all simulations
  scaledSVAR <- bind_rows(scaledSVAR,svarScale_tmp)
}
```


```{r "plot-scaled-svar-CI-model-1", echo = FALSE, fig.align = 'center', fig.width = 5, fig.height=6}
ToPlotCI <- scaledSVAR %>%  filter(K %in% c(230, 1000) & nRSloops %in% c(230, 1000)) %>% 
  group_by(K, nRSloops) %>%
  slice(seq(1, nsim, length.out = 100)) %>% ungroup()
ToPlotCI$K <- factor(ToPlotCI$K, levels = c(230, 1000), labels = c('K = 230', 'K = 1000'))
ToPlotCI$nRSloops <- factor(ToPlotCI$nRSloops, levels = c(230, 1000), labels = c('B = 230', 'B = 1000'))

ggplot(ToPlotCI, aes(x = TrueBeta)) + geom_vline(aes(xintercept = TrueBeta), size = .8, alpha = .8) +
   geom_point(aes(x = AvgBeta, y = sim, colour = factor(coverage_ind)), size = 0.5) + 
   geom_segment(aes(x = CIlow, xend = CIup, y = sim, yend = sim, colour = factor(coverage_ind), alpha = factor(coverage_ind)), size = 0.9) + 
  facet_wrap(K ~ nRSloops, dir = 'v') +
  scale_alpha_manual("Contains true value", values = c(1,0.8), labels = c("NO", "YES")) +
  scale_colour_manual("Contains true value", values = c('#d95f02', '#1b9e77'), labels = c("NO", "YES")) +
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
  theme(legend.position="bottom") +
   ggtitle("100 random selected simulations with the 95% CI", subtitle = "scaled mean sandwich variance estimator")
```

```{r "table-scaled-svar-model-1"}
scaledSVARtable_tmp <- scaledSVAR %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
scaledSVARtable <- matrix(scaledSVARtable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(scaledSVARtable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(scaledSVARtable) <- paste('K = ', K_vec, sep = '')
knitr::kable(scaledSVARtable)
```


##### Sandwich variance estimator CI

After talking with Jan, it should really be:

$$
\left[ \widehat{\beta_{final}} \pm z_{1-\alpha/2} \times \sqrt{\frac{1}{B^2} \sum_{i=1}^B \widehat{\text{Var}(\beta_i)}} \right]
$$
As you have B parts of K datapoints. To get $\widehat{\beta}_{final}$, which is the end estimate, you take the mean of the B estimates. Hence you sum and divide by $B$. The variance of a constant times the sum equals the squared constant times the sum of the variance. If, the B parts are independent!
<br>
However, in our subsampling scheme, this is not the case! So I expect some worse coverages here, but a better coverage in the bag of m-ou-of-n bootstrap procedure. 

```{r "calculate-correct-sandwich-variance-CI-model-1", cache = TRUE}
SummedSvar <- data.frame() %>% tbl_df()
# Read in data
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
        tbl_df()
  # CI through now summing sandwich variance of beta
  SummedSvar_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    SummedSvar = sum(sVariance, na.rm = TRUE)) %>%
          mutate(bVar = SummedSvar * (1/nRSloops^2),
              CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) *
                                       sqrt(bVar)),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * 
                                  sqrt(bVar)),
              type = 'avSVar',  sim = i,
              coverage_ind = 
                ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0))
  
  # Collect the values over all simulations
  SummedSvar <- bind_rows(SummedSvar,SummedSvar_tmp)
}
```


```{r "plot-correct-svar-CI-model-1", echo = FALSE, fig.align = 'center', fig.width = 5, fig.height=6}
ToPlotCI <- SummedSvar %>%  filter(K %in% c(230, 1000) & nRSloops %in% c(230, 1000)) %>% 
  group_by(K, nRSloops) %>%
  slice(seq(1, nsim, length.out = 100)) %>% ungroup()
ToPlotCI$K <- factor(ToPlotCI$K, levels = c(230, 1000), labels = c('K = 230', 'K = 1000'))
ToPlotCI$nRSloops <- factor(ToPlotCI$nRSloops, levels = c(230, 1000), labels = c('B = 230', 'B = 1000'))

ggplot(ToPlotCI, aes(x = TrueBeta)) + 
  geom_vline(aes(xintercept = TrueBeta), size = .8, alpha = .8) +
   geom_point(aes(x = AvgBeta, y = sim, colour = factor(coverage_ind)), size = 0.5) + 
   geom_segment(aes(x = CIlow, xend = CIup, y = sim, yend = sim, colour = factor(coverage_ind), alpha = factor(coverage_ind)), size = 0.9) + 
  facet_wrap(K ~ nRSloops, dir = 'v') +
  scale_alpha_manual("Contains true value", values = c(1,0.8), labels = c("NO", "YES")) +
  scale_colour_manual("Contains true value", values = c('#d95f02', '#1b9e77'), labels = c("NO", "YES")) +
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
  theme(legend.position="bottom") +
   ggtitle("100 random selected simulations with the 95% CI", subtitle = "correct mean sandwich variance estimator")
```

```{r "table-correct-svar-model-1"}
SummedSvartable_tmp <- SummedSvar %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
SummedSvartable <- matrix(SummedSvartable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(SummedSvartable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(SummedSvartable) <- paste('K = ', K_vec, sep = '')
knitr::kable(SummedSvartable)
```


### Model 2

Parameters:
```{r "model-2-parameters", cache.rebuild = TRUE}
u <- 10
alpha <- 10
sigma <- 5
trueBeta <- alpha/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 2
```

#### Load in data

Start with estimated $\beta$:
```{r echo = FALSE}
# Data frame with all values over 1000 simulations
valuesAllSim <- timeAllSim <- data.frame() %>% tbl_df()

# Progress
progress <- floor(seq(1,nsim,length.out = 11)[-1])
```


```{r "load-beta-model-2", results = 'hide', cache = TRUE}
# load in estimated beta values
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
               col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), 
               header = TRUE) %>% tbl_df() %>%
      group_by(K, nRSloops) %>% summarise(avBeta = mean(beta)) %>% 
      mutate(sim = i), silent = TRUE)
  if(class(ValSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  valuesAllSim <- bind_rows(valuesAllSim, ValSim)
}
```

Same for computational time:
```{r "load-time-model-2", results = 'hide', cache = TRUE}
# load in computational time
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: computational time 
  TimeSim <-  try(read.table(file = paste0(datLoc, 'uni_time_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
              col.names = c("minutes"), header = TRUE) %>% tbl_df() %>%
              bind_cols(., pairs) %>% 
              mutate(sim = i), silent = TRUE)
    if(class(TimeSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  timeAllSim <- bind_rows(timeAllSim, TimeSim)
}
```


#### Bias vs computational time

Quick summary:
```{r collapse = TRUE}
tbl_df(valuesAllSim)
group_by(valuesAllSim, K, nRSloops) %>% summarise(avBeta = mean(avBeta)) %>% summary()
```

Plotting the estimated $\beta$ parameters for each simulation through facets on the number of iterations (**B**).

```{r "beta-model-2", echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = K, y = avBeta)) + 
  geom_point(size = 0.7, colour = "#67a9cf") + 
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = expression(hat(beta))) +
  geom_line(data = mutate(.data = valuesAllSim, TrueValue = trueBeta, Label = "True Value of Beta"),
             aes(y = TrueValue, linetype = Label), size = 0.7, show.legend = TRUE) +
  scale_linetype_discrete(name = "") +
  ggtitle("Estimated beta parameter given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

Plotting the computational time.

```{r "time-model-2", echo=FALSE, fig.align="center"}
TimeToPlot <- timeAllSim %>% group_by(B,K) %>% summarise(AvgMin = mean(minutes))
  TimeToPlot$B <- factor(TimeToPlot$B)
  TimeToPlot$K <- factor(TimeToPlot$K)

ggplot(data = TimeToPlot, aes(x = K, y = AvgMin, group = B)) + 
  geom_line(aes(colour = B),size = 1) +
  scale_y_continuous(name = "Minutes",
                     limits = c(0,max(TimeToPlot$AvgMin) + 2)) + 
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Average computational time to estimate PIM on HPC") +
  theme_minimal()
```

> These data generating parameters do not work!

### Model 3

Parameters:
```{r "model-3-parameters", cache.rebuild = TRUE}
alpha_1 <- -0.43
# Sigma based on dataset
sigma <- 9.51
trueBeta <- alpha_1/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 3
```

#### Load in data

Start with estimated $\beta$:
```{r echo = FALSE}
# Data frame with all values over 1000 simulations
valuesAllSim <- timeAllSim <- data.frame() %>% tbl_df()

# Progress
progress <- floor(seq(1,nsim,length.out = 11)[-1])
```


```{r "load-beta-model-3", results = 'hide', cache = TRUE}
S3_colnames <- c("beta.X_smartph_hrs", "beta.X_sex", 
                 "beta.X_econArea",  "beta.X_ethnic", 
                 "sVariance.X_smartph_hrs", "sVariance.X_sex",
                 "sVariance.X_econArea", 
                 "sVariance.X_ethnic",  "K", "nRSloops", "TrueBeta")

# load in estimated beta values
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_3.txt'),
                        col.names = S3_colnames, header = TRUE) %>% tbl_df()  %>%
        group_by(K, nRSloops) %>% summarise_all(mean) %>%
        mutate(sim = i)
  
   if(class(ValSim) == "try-error"){
    print(i);next
  }
  
  # Add to data frame with all simulations
  valuesAllSim <- bind_rows(valuesAllSim, ValSim)
}
```

Same for computational time:
```{r "load-time-model-3", results = 'hide', cache.rebuild = TRUE}
# load in computational time
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: computational time 
  TimeSim <-  read.table(file = paste0(datLoc, 'uni_time_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
              col.names = c("minutes"), header = TRUE) %>% tbl_df() %>%
              bind_cols(., pairs) %>% 
              mutate(sim = i)
  
  # Add to data frame with all simulations
  timeAllSim <- bind_rows(timeAllSim, TimeSim)
}
```

#### Bias vs computational time

Quick summary:
```{r collapse = TRUE}
group_by(valuesAllSim, K, nRSloops) %>% summarise(avBeta = mean(beta.X_smartph_hrs)) %>% summary()
```

Plotting the estimated $\beta$ parameters for each simulation through facetting on the number of iterations (**B**).

```{r "beta-model-3", echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = K, y = beta.X_smartph_hrs)) + 
  geom_point(size = 0.7, colour = "#67a9cf") + 
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = expression(hat(beta))) +
  geom_line(data = mutate(.data = valuesAllSim, TrueValue = TrueBeta, Label = "True Value of Beta"),
             aes(y = TrueValue, linetype = Label), size = 0.7, show.legend = TRUE) +
  scale_linetype_discrete(name = "") +
  ggtitle("Estimated beta parameter given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

```{r "calculate-MSE-model-3"}
MSEtable_tmp <- group_by(valuesAllSim, K, nRSloops) %>% mutate(SE = (beta.X_smartph_hrs - TrueBeta)^2) %>%
  summarise(MSE = mean(SE, na.rm = TRUE))
MSEtable <- matrix(MSEtable_tmp$MSE, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(MSEtable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(MSEtable) <- paste('K = ', K_vec, sep = '')
options( scipen = 6 )
knitr::kable(MSEtable)
```

Plotting the computational time.

```{r "time-model-3", echo=FALSE, fig.align="center"}
TimeToPlot <- timeAllSim %>% group_by(B,K) %>% summarise(AvgMin = mean(minutes))
  TimeToPlot$B <- factor(TimeToPlot$B)
  TimeToPlot$K <- factor(TimeToPlot$K)

ggplot(data = TimeToPlot, aes(x = K, y = AvgMin, group = B)) + 
  geom_line(aes(colour = B),size = 1) +
  scale_y_continuous(name = "Minutes",
                     limits = c(0,max(TimeToPlot$AvgMin) + 2)) + 
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Average computational time to estimate PIM on HPC") +
  theme_minimal()
```

#### Distribution of estimator

```{r "QQplot-model-3", fig.show='hold', fig.width=10}
for(j in 1:length(nRSloops_vec)){
  K_temp <- K_vec[j]
  B_temp <- nRSloops_vec[j]
  assign(paste0("Plot", j),
    valuesAllSim %>% filter(K == K_temp & nRSloops == B_temp) %>% 
    select(beta.X_smartph_hrs) %>% unlist(.) %>% 
    as.numeric() %>% gg_qq(x = ., title = paste0('K = ', K_temp, ' and B = ', B_temp))
  )
}

cowplot::plot_grid(Plot1, Plot2, Plot3, Plot4, Plot5, Plot6, labels = c("A", "B", "C", "D", "E", "F"), ncol = 3)
cowplot::plot_grid(Plot7, Plot8, Plot9, Plot10, labels = c("G", "H", "I", "J"), ncol = 3)
```

#### Emperical CI coverage

##### Bias-corrected bootstrap based CI

Same procedure as model 1:

```{r "percentile-CI-model-3-parallel", results = 'hide'}
# Detect and start the workers
P <- detectCores(logical = FALSE) # physical cores
cl <- makeCluster(P)

# Initialize them with the libraries
clusterEvalQ(cl, library(tidyverse))
clusterEvalQ(cl, library(boot))

# Run in parallel and append results
boot_par_results <- clusterApply(cl, 1:nsim, fun = BootParCI, datLoc = datLoc, SCEN = SCEN)
boot_CI_coverage <- do.call(rbind, boot_par_results)

# Stop the workers
stopCluster(cl)
```

```{r "table-EC-bootstrap-model-3", echo = FALSE, cache.rebuild = TRUE}
resTable_tmp <- boot_CI_coverage %>% ungroup() %>% group_by(K, nRSloops) %>% summarise(EC = round(mean(coverage), 3))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```

> Somewhat better.


##### Student - t confidence intervals


```{r "calculate-student-t-CI-model-3"}
norm_tAvg <- c()
# Read in data (again, sigh)
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = S3_colnames, header = TRUE) %>% 
        tbl_df()
  ValSim <- rename(ValSim, beta = beta.X_smartph_hrs)
  # CI using SD of data and normal quantiles and using t-quantiles
    # Formula = sd/sqrt(n) for some strange reason (I expected sd of beta being equal to se)
  norm_tmp <- ValSim %>% group_by(K, nRSloops) %>% 
          mutate(
          CIlow = beta - (qnorm(0.025, lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          CIup = beta + (qnorm(0.025, lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          type = 'norm') %>% group_by(K, nRSloops, TrueBeta, type) %>%
    summarise(AvgBeta = mean(beta, na.rm = TRUE),
          AvgLow = mean(CIlow, na.rm = TRUE), 
          AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup() %>% rowwise() %>% 
    mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0),
          sim = i)
  t_tmp <- ValSim %>% group_by(K, nRSloops) %>% 
          mutate(
          CIlow = beta - (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          CIup = beta + (qt(0.025, df = (K - 1), lower.tail = FALSE) * (sd(beta)/(sqrt(K - 1)))),
          type = 't') %>% group_by(K, nRSloops, TrueBeta, type) %>%
    summarise(AvgBeta = mean(beta, na.rm = TRUE),
          AvgLow = mean(CIlow, na.rm = TRUE), 
          AvgUp = mean(CIup, na.rm = TRUE)) %>% ungroup() %>% rowwise() %>% 
    mutate(coverage_ind = ifelse(TrueBeta >= AvgLow && TrueBeta <= AvgUp, 1, 0),
          sim = i)
  
  # Construct general CI by averaging endpoints
  norm_tAvg <- bind_rows(norm_tAvg,norm_tmp, t_tmp)
}
```

```{r "visualize-student-t-CI-model-3", fig.align = 'center'}
# Plot
norm_tAvg %>% filter(type == 't' & K == 120 & nRSloops == 120) %>% 
  slice(round(seq(1,nsim,length.out = 50),0)) %>%
   ggplot(., aes(x = TrueBeta)) + geom_vline(aes(xintercept = TrueBeta)) +
   geom_point(aes(x = AvgBeta, y = sim)) + 
   geom_segment(aes(x = AvgLow, xend = AvgUp, y = sim, yend = sim)) + 
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
   ggtitle("95% CI around beta PIM estimate")

resTable_tmp <- norm_tAvg %>% filter(type == 't') %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
resTable <- matrix(resTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(resTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(resTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(resTable)
```

##### Bootstrap m-out-of n

Again, scale the CI according to bootstrap m-out-of n theory.

```{r "calculate-m-out-of-n-CI-model-3", cache = TRUE}
scaledSD <- data.frame() %>% tbl_df()
# Read in data (again, sigh)
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = S3_colnames, header = TRUE) %>% 
        tbl_df()
  ValSim <- rename(ValSim, beta = beta.X_smartph_hrs)
  # CI using SD of beta with scaling
  sdScale_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    sdBeta = sd(beta, na.rm = TRUE)) %>%
          mutate(CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) * sdBeta * sqrt(K/n)),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * sdBeta * sqrt(K/n)),
              type = 'scaledSD') %>% 
          ungroup() %>% rowwise() %>% 
          mutate(coverage_ind = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0),
              sim = i)
  
  # Collect the values over all simulations
  scaledSD <- bind_rows(scaledSD,sdScale_tmp)
}
```


```{r "plot-CI-model-3", echo = FALSE, fig.align = 'center', fig.width = 5, fig.height=6, cache.rebuild=TRUE}
ToPlotCI <- scaledSD %>%  filter(K %in% c(230, 1000) & nRSloops %in% c(230, 1000)) %>% 
  group_by(K, nRSloops) %>%
  slice(seq(1, nsim, length.out = 100)) %>% ungroup()
ToPlotCI$K <- factor(ToPlotCI$K, levels = c(230, 1000), labels = c('K = 230', 'K = 1000'))
ToPlotCI$nRSloops <- factor(ToPlotCI$nRSloops, levels = c(230, 1000), labels = c('B = 230', 'B = 1000'))

ggplot(ToPlotCI, aes(x = TrueBeta)) + geom_vline(aes(xintercept = TrueBeta), size = .8, alpha = .8) +
   geom_point(aes(x = AvgBeta, y = sim, colour = factor(coverage_ind)), size = 0.5) + 
   geom_segment(aes(x = CIlow, xend = CIup, y = sim, yend = sim, colour = factor(coverage_ind), alpha = factor(coverage_ind)), size = 0.9) + 
  facet_wrap(K ~ nRSloops, dir = 'v') +
  scale_alpha_manual("Contains true value", values = c(1,0.8), labels = c("NO", "YES")) +
  scale_colour_manual("Contains true value", values = c('#d95f02', '#1b9e77'), labels = c("NO", "YES")) +
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
  theme(legend.position="bottom") +
   ggtitle("100 random selected simulations with the 95% CI")
```

```{r "table-scaled-sd-model-3", cache.rebuild = TRUE}
scaledSDTable_tmp <- scaledSD %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
scaledSDTable <- matrix(scaledSDTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(scaledSDTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(scaledSDTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(scaledSDTable)
```

##### Sandwich variance estimator CI


```{r "calculate-correct-sandwich-variance-CI-model-3", cache = TRUE}
S3_colnames <- c("beta.X_smartph_hrs", "beta.X_sex", 
                 "beta.X_econArea",  "beta.X_ethnic", 
                 "sVariance.X_smartph_hrs", "sVariance.X_sex",
                 "sVariance.X_econArea", 
                 "sVariance.X_ethnic",  "K", "nRSloops", "TrueBeta")

SummedSvar <- data.frame() %>% tbl_df()
# Read in data
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = S3_colnames, header = TRUE) %>% 
        tbl_df()
  ValSim <- rename(ValSim, beta = beta.X_smartph_hrs, sVariance = sVariance.X_smartph_hrs)
  # CI through now summing sandwich variance of beta
  SummedSvar_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    SummedSvar = sum(sVariance, na.rm = TRUE)) %>%
          mutate(bVar = SummedSvar * (1/nRSloops^2),
              CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) *
                                       sqrt(bVar)),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * 
                                  sqrt(bVar)),
              type = 'avSVar',  sim = i,
              coverage_ind = 
                ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0))
  
  # Collect the values over all simulations
  SummedSvar <- bind_rows(SummedSvar,SummedSvar_tmp)
}
```


```{r "plot-correct-svar-CI-model-3", echo = FALSE, fig.align = 'center', fig.width = 5, fig.height=6}
ToPlotCI <- SummedSvar %>%  filter(K %in% c(230, 1000) & nRSloops %in% c(230, 1000)) %>% 
  group_by(K, nRSloops) %>%
  slice(seq(1, nsim, length.out = 100)) %>% ungroup()
ToPlotCI$K <- factor(ToPlotCI$K, levels = c(230, 1000), labels = c('K = 230', 'K = 1000'))
ToPlotCI$nRSloops <- factor(ToPlotCI$nRSloops, levels = c(230, 1000), labels = c('B = 230', 'B = 1000'))

ggplot(ToPlotCI, aes(x = TrueBeta)) + 
  geom_vline(aes(xintercept = TrueBeta), size = .8, alpha = .8) +
   geom_point(aes(x = AvgBeta, y = sim, colour = factor(coverage_ind)), size = 0.5) + 
   geom_segment(aes(x = CIlow, xend = CIup, y = sim, yend = sim, colour = factor(coverage_ind), alpha = factor(coverage_ind)), size = 0.9) + 
  facet_wrap(K ~ nRSloops, dir = 'v') +
  scale_alpha_manual("Contains true value", values = c(1,0.8), labels = c("NO", "YES")) +
  scale_colour_manual("Contains true value", values = c('#d95f02', '#1b9e77'), labels = c("NO", "YES")) +
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
  theme(legend.position="bottom") +
   ggtitle("100 random selected simulations with the 95% CI", subtitle = "Average sandwich variance estimator")
```

```{r "table-correct-svar-model-3"}
SummedSvartable_tmp <- SummedSvar %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
SummedSvartable <- matrix(SummedSvartable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(SummedSvartable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(SummedSvartable) <- paste('K = ', K_vec, sep = '')
knitr::kable(SummedSvartable)
```


### Model 4

Parameters:
```{r "model-4-parameters", cache.rebuild = TRUE}
u <- 10
alpha <- 1
sigma <- 5
trueBeta <- alpha/(sqrt(2) * sigma)
# Model (called SCEN in script)
SCEN <- 4
```

#### Load in data

Start with estimated $\beta$:
```{r echo = FALSE}
# Data frame with all values over all simulations
valuesAllSim <- timeAllSim <- data.frame() %>% tbl_df()

# Progress
progress <- floor(seq(1,nsim,length.out = 11)[-1])
```


```{r "load-beta-model-4", results = 'hide', cache = TRUE}
# load in estimated beta values
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: estimated beta values averaged within the sampling algorithm
  ValSim <-  try(read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
               col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), 
               header = TRUE) %>% tbl_df() %>%
      group_by(K, nRSloops, TrueBeta) %>% summarise(avBeta = mean(beta)) %>% 
      mutate(sim = i), silent = TRUE)
  if(class(ValSim)[1] == "try-error"){
    print(i);next
  }
  # Add to data frame with all simulations
  valuesAllSim <- bind_rows(valuesAllSim, ValSim)
}
```

Same for computational time:
```{r "load-time-model-4", results = 'hide', cache = TRUE}
# load in computational time
for(i in 1:nsim){
  if(i %in% progress) print(paste0("At ", i/nsim*100, "%"))
  # Values in one simulation: computational time 
  TimeSim <-  try(read.table(file = paste0(datLoc, 'uni_time_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
              col.names = c("minutes"), header = TRUE) %>% tbl_df() %>%
              bind_cols(., pairs) %>% 
              mutate(sim = i), silent = TRUE)
    if(class(TimeSim)[1] == "try-error"){
    print(i);next
  }
  # Add to data frame with all simulations
  timeAllSim <- bind_rows(timeAllSim, TimeSim)
}
```

#### Bias vs computational time

Quick summary:
```{r collapse = TRUE}
tbl_df(valuesAllSim)
group_by(valuesAllSim, K, nRSloops) %>% summarise(avBeta = mean(avBeta)) %>% summary()
```

Plotting the estimated $\beta$ parameters for each simulation through facets on the number of iterations (**B**).

```{r "beta-model-4", echo=FALSE, fig.align="center"}
ggplot(valuesAllSim, aes(x = K, y = avBeta)) + 
  geom_point(size = 0.7, colour = "#67a9cf") + 
  facet_grid(. ~ factor(nRSloops)) +
  scale_x_continuous(name = "Number of samples (K) in one iteration",
                     breaks = seq(min(pairs$K), max(pairs$K),length.out = 4)) +
  scale_y_continuous(name = expression(hat(beta))) +
  geom_line(data = mutate(.data = valuesAllSim, TrueValue = TrueBeta, Label = "True Value of Beta"),
             aes(y = TrueValue, linetype = Label), size = 0.7, show.legend = TRUE) +
  scale_linetype_discrete(name = "") +
  ggtitle("Estimated beta parameter given the number of iterations (B)") +
  theme_minimal() +
  theme(axis.text.x=element_text(angle = -75, hjust = 0),
        legend.position = "bottom")
```

```{r "calculate-MSE-model-4"}
MSEtable_tmp <- group_by(valuesAllSim, K, nRSloops) %>% mutate(SE = (avBeta - TrueBeta)^2) %>%
  summarise(MSE = mean(SE, na.rm = TRUE))
MSEtable <- matrix(MSEtable_tmp$MSE, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(MSEtable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(MSEtable) <- paste('K = ', K_vec, sep = '')
options( scipen = 6 )
knitr::kable(MSEtable)
```

Plotting the computational time.

```{r "time-model-4", echo=FALSE, fig.align="center"}
TimeToPlot <- timeAllSim %>% group_by(B,K) %>% summarise(AvgMin = mean(minutes))
  TimeToPlot$B <- factor(TimeToPlot$B)
  TimeToPlot$K <- factor(TimeToPlot$K)

ggplot(data = TimeToPlot, aes(x = K, y = AvgMin, group = B)) + 
  geom_line(aes(colour = B),size = 1) +
  scale_y_continuous(name = "Minutes",
                     limits = c(0,max(TimeToPlot$AvgMin) + 2)) + 
  scale_x_discrete(name = "Number of samples (K) in one iteration") +
  scale_color_brewer(name = "Number of \niterations (B)",
                     type = "diverging", palette = "RdYlBu") +
  guides(colour = guide_legend(override.aes = list(size=2))) + 
  ggtitle("Average computational time to estimate PIM on HPC") +
  theme_minimal()
```

#### Distribution of estimator

```{r "QQplot-model-4", fig.show='hold', fig.width=10}
for(j in 1:length(nRSloops_vec)){
  K_temp <- K_vec[j]
  B_temp <- nRSloops_vec[j]
  assign(paste0("Plot", j),
    valuesAllSim %>% filter(K == K_temp & nRSloops == B_temp) %>% 
    select(avBeta) %>% unlist(.) %>% 
    as.numeric() %>% gg_qq(x = ., title = paste0('K = ', K_temp, ' and B = ', B_temp))
  )
}

cowplot::plot_grid(Plot1, Plot2, Plot3, Plot4, Plot5, Plot6, labels = c("A", "B", "C", "D", "E", "F"), ncol = 3)
cowplot::plot_grid(Plot7, Plot8, Plot9, Plot10, labels = c("G", "H", "I", "J"), ncol = 3)
```


#### Emperical CI coverage

##### Bootstrap m-out-of n

Again, scale the CI according to bootstrap m-out-of n theory.

```{r "calculate-m-out-of-n-CI-model-4", cache = TRUE}
scaledSD <- data.frame() %>% tbl_df()
# Read in data (again, sigh)
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), 
         header = TRUE) %>% 
          tbl_df()
  # CI using SD of beta with scaling
  sdScale_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    sdBeta = sd(beta, na.rm = TRUE)) %>%
          mutate(CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) * sdBeta * sqrt(K/n)),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * sdBeta * sqrt(K/n)),
              type = 'scaledSD') %>% 
          ungroup() %>% rowwise() %>% 
          mutate(coverage_ind = ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0),
              sim = i)
  # Collect the values over all simulations
  scaledSD <- bind_rows(scaledSD,sdScale_tmp)
}
```


```{r "plot-CI-model-4", echo = FALSE, fig.align = 'center', fig.width = 5, fig.height=6}
ToPlotCI <- scaledSD %>%  filter(K %in% c(230, 1000) & nRSloops %in% c(230, 1000)) %>% 
  group_by(K, nRSloops) %>%
  slice(seq(1, nsim, length.out = 100)) %>% ungroup()
ToPlotCI$K <- factor(ToPlotCI$K, levels = c(230, 1000), labels = c('K = 230', 'K = 1000'))
ToPlotCI$nRSloops <- factor(ToPlotCI$nRSloops, levels = c(230, 1000), labels = c('B = 230', 'B = 1000'))

ggplot(ToPlotCI, aes(x = TrueBeta)) + geom_vline(aes(xintercept = TrueBeta), size = .8, alpha = .8) +
   geom_point(aes(x = AvgBeta, y = sim, colour = factor(coverage_ind)), size = 0.5) + 
   geom_segment(aes(x = CIlow, xend = CIup, y = sim, yend = sim, colour = factor(coverage_ind), alpha = factor(coverage_ind)), size = 0.9) + 
  facet_wrap(K ~ nRSloops, dir = 'v') +
  scale_alpha_manual("Contains true value", values = c(1,0.8), labels = c("NO", "YES")) +
  scale_colour_manual("Contains true value", values = c('#d95f02', '#1b9e77'), labels = c("NO", "YES")) +
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
  theme(legend.position="bottom") +
   ggtitle("100 random selected simulations with the 95% CI")
```

```{r "table-scaled-sd-model-4", cache.rebuild = TRUE}
scaledSDTable_tmp <- scaledSD %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
scaledSDTable <- matrix(scaledSDTable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(scaledSDTable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(scaledSDTable) <- paste('K = ', K_vec, sep = '')
knitr::kable(scaledSDTable)
```

##### Sandwich variance estimator CI

```{r "calculate-correct-sandwich-variance-CI-model-4", cache = TRUE}
SummedSvar <- data.frame() %>% tbl_df()
# Read in data
for(i in 1:nsim){
  ValSim <- read.table(file = paste0(datLoc, 'uni_beta_vector_simID_', i, '_SCEN_', SCEN, '.txt'),
         col.names = c("beta", "sVariance", "K", "nRSloops", "TrueBeta"), header = TRUE) %>% 
        tbl_df()
  # CI through now summing sandwich variance of beta
  SummedSvar_tmp <- ValSim %>% group_by(K, nRSloops, TrueBeta) %>% 
          summarise(AvgBeta = mean(beta, na.rm = TRUE),  
                    SummedSvar = sum(sVariance, na.rm = TRUE)) %>%
          mutate(bVar = SummedSvar * (1/nRSloops^2),
              CIlow =  AvgBeta - (qnorm(0.025, lower.tail = FALSE) *
                                       sqrt(bVar)),
              CIup = AvgBeta + (qnorm(0.025, lower.tail = FALSE) * 
                                  sqrt(bVar)),
              type = 'avSVar',  sim = i,
              coverage_ind = 
                ifelse(TrueBeta >= CIlow && TrueBeta <= CIup, 1, 0))
  
  # Collect the values over all simulations
  SummedSvar <- bind_rows(SummedSvar,SummedSvar_tmp)
}
```

```{r "plot-correct-svar-CI-model-4", echo = FALSE, fig.align = 'center', fig.width = 5, fig.height=6}
ToPlotCI <- SummedSvar %>%  filter(K %in% c(230, 1000) & nRSloops %in% c(230, 1000)) %>% 
  group_by(K, nRSloops) %>%
  slice(seq(1, nsim, length.out = 100)) %>% ungroup()
ToPlotCI$K <- factor(ToPlotCI$K, levels = c(230, 1000), labels = c('K = 230', 'K = 1000'))
ToPlotCI$nRSloops <- factor(ToPlotCI$nRSloops, levels = c(230, 1000), labels = c('B = 230', 'B = 1000'))

ggplot(ToPlotCI, aes(x = TrueBeta)) + 
  geom_vline(aes(xintercept = TrueBeta), size = .8, alpha = .8) +
   geom_point(aes(x = AvgBeta, y = sim, colour = factor(coverage_ind)), size = 0.5) + 
   geom_segment(aes(x = CIlow, xend = CIup, y = sim, yend = sim, colour = factor(coverage_ind), alpha = factor(coverage_ind)), size = 0.9) + 
  facet_wrap(K ~ nRSloops, dir = 'v') +
  scale_alpha_manual("Contains true value", values = c(1,0.8), labels = c("NO", "YES")) +
  scale_colour_manual("Contains true value", values = c('#d95f02', '#1b9e77'), labels = c("NO", "YES")) +
   scale_x_continuous("beta") + scale_y_continuous("simulation") +
  theme(legend.position="bottom") +
   ggtitle("100 random selected simulations with the 95% CI", subtitle = "Average sandwich variance estimator")
```

```{r "table-correct-svar-model-4"}
SummedSvartable_tmp <- SummedSvar %>% group_by(K,nRSloops) %>% summarise(EC = mean(coverage_ind))
SummedSvartable <- matrix(SummedSvartable_tmp$EC, ncol = length(nRSloops_vec), byrow = TRUE)
colnames(SummedSvartable) <- paste('B = ', nRSloops_vec, sep = '')
rownames(SummedSvartable) <- paste('K = ', K_vec, sep = '')
knitr::kable(SummedSvartable)
```
